{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "from torch import cuda\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dlordinal.datasets import FGNet\n",
    "from dlordinal.losses import OrdinalECOCDistanceLoss\n",
    "from dlordinal.wrappers import OBDECOCModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess of FGNet dataset\n",
    "\n",
    "First, we present the configuration parameters for the experimentation and the number of workers for the *DataLoader*, which defines the number of subprocesses to use for data loading. In this specific case, it refers to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_params = {\"lr\": 1e-3, \"bs\": 200, \"epochs\": 5, \"s\": 2, \"c\": 0.2, \"beta\": 0.5}\n",
    "\n",
    "workers = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the *FGNet* method to download and preprocess the images. Once that is done with the training data, we create a validation partition comprising 15% of the data using the *StratifiedShuffleSplit* method. Finally, with all the partitions, we load the images using a method called *DataLoader*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected image shape: [3, 128, 128]\n",
      "class_weights=tensor([1.0191, 1.5345, 0.7946, 1.1314, 0.5517, 2.4273])\n"
     ]
    }
   ],
   "source": [
    "fgnet_trainval = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "test_data = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "num_classes = len(fgnet_trainval.classes)\n",
    "classes = fgnet_trainval.classes\n",
    "targets = fgnet_trainval.targets\n",
    "\n",
    "# Create a validation split\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=0)\n",
    "sss_splits = list(sss.split(X=np.zeros(len(fgnet_trainval)), y=fgnet_trainval.targets))\n",
    "train_idx, val_idx = sss_splits[0]\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_data = Subset(fgnet_trainval, train_idx)\n",
    "val_data = Subset(fgnet_trainval, val_idx)\n",
    "\n",
    "# Get CUDA device\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=optimiser_params[\"bs\"], shuffle=True, num_workers=workers\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_data, batch_size=optimiser_params[\"bs\"], shuffle=True, num_workers=workers\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=optimiser_params[\"bs\"], shuffle=False, num_workers=workers\n",
    ")\n",
    "\n",
    "# Get image shape\n",
    "img_shape = None\n",
    "for X, _ in train_dataloader:\n",
    "    img_shape = list(X.shape[1:])\n",
    "    break\n",
    "print(f\"Detected image shape: {img_shape}\")\n",
    "\n",
    "# Define class weights for imbalanced datasets\n",
    "classes_array = np.array([int(c) for c in classes])\n",
    "\n",
    "class_weights = (\n",
    "    torch.from_numpy(\n",
    "        class_weight.compute_class_weight(\"balanced\", classes=classes_array, y=targets)\n",
    "    )\n",
    "    .float()\n",
    "    .to(device)\n",
    ")\n",
    "print(f\"{class_weights=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and optimiser\n",
    "\n",
    "We are using a modified version of the *ResNet* architecture, specifically designed for the loss function explained in the next section. In this case, the *ResNet* model is not pretrained with *ImageNet*, so we will need to undergo an extensive learning process.\n",
    "\n",
    "To adapt the outputs of the model to this, the final fully-connected block is substituted by $Q-1$ fully-connected blocks [1], each one with a single output unit with sigmoid activation.\n",
    "\n",
    "As an alternative to the *ResNet* architecture, you can use the *VGG* architecture, which has also been implemented to work with the loss function explained in the following sections.\n",
    "\n",
    "Finally, we define the *Adam* optimiser, which is used to adjust the network's weights and minimize the error of a loss function.\n",
    "\n",
    "[1]: Barbero-Gómez, J., Gutiérrez, P. A., & Hervás-Martínez, C. (2022). *Error-correcting output codes in the framework of deep ordinal classification.* Neural Processing Letters, 1-32. Springer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OBDECOCModel(num_classes, resnet18(num_classes=1000), base_n_outputs=1000).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=optimiser_params[\"lr\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The original $Q$-class ordinal problem is decomposed into $Q-1$ binary decision problems, what is known as Ordinal Binary Decomposition (ODB) [1]. So the categorical cross-entropy has been substituted by the Mean Squared Error loss because it copes better with the distance function used for the Error-Conecting Output Codes (ECOC) decision:\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{N} ∑_{i=1}^N ∑_{k=1}^{Q-1} (\\mathbf{1} \\{y_i \\succ \\mathcal{C}_k\\} - P(y_i \\succ \\mathcal{C}_k | x_i))^2\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1} \\{y_i \\succ \\mathcal{C}_k\\}$ is the indicator function that is equal to 1 when $y_i \\succ \\mathcal{C}_k$ and 0 otherwise, and $P(y_i \\succ \\mathcal{C}_k | x_i)$ is the probability that $y_i \\succ \\mathcal{C}_k$ predicted by the network given a sample.\n",
    "\n",
    "[1]: Barbero-Gómez, J., Gutiérrez, P. A., & Hervás-Martínez, C. (2022). *Error-correcting output codes in the framework of deep ordinal classification.* Neural Processing Letters, 1-32. Springer. doi.org/10.1007/s11063-022-10824-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = OrdinalECOCDistanceLoss(num_classes=num_classes, weights=class_weights).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics computation\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, num_classes: int):\n",
    "\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    labels = range(0, num_classes)\n",
    "\n",
    "    # Metrics calculation\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\", labels=labels)\n",
    "    ms = minimum_sensitivity(y_true, y_pred, labels=labels)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    off1 = accuracy_off1(y_true, y_pred, labels=labels)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"QWK\": qwk,\n",
    "        \"MS\": ms,\n",
    "        \"MAE\": mae,\n",
    "        \"CCR\": acc,\n",
    "        \"1-off\": off1,\n",
    "        \"Confusion matrix\": conf_mat,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _compute_sensitivities(y_true, y_pred, labels=None):\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    sum = np.sum(conf_mat, axis=1)\n",
    "    mask = np.eye(conf_mat.shape[0], conf_mat.shape[1])\n",
    "    correct = np.sum(conf_mat * mask, axis=1)\n",
    "    sensitivities = correct / sum\n",
    "\n",
    "    sensitivities = sensitivities[~np.isnan(sensitivities)]\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "\n",
    "def minimum_sensitivity(y_true, y_pred, labels=None):\n",
    "    return np.min(_compute_sensitivities(y_true, y_pred, labels=labels))\n",
    "\n",
    "\n",
    "def accuracy_off1(y_true, y_pred, labels=None):\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    n = conf_mat.shape[0]\n",
    "    mask = np.eye(n, n) + np.eye(n, n, k=1), +np.eye(n, n, k=-1)\n",
    "    correct = mask * conf_mat\n",
    "\n",
    "    return 1.0 * np.sum(correct) / np.sum(conf_mat)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(\"\")\n",
    "    print(\"Confusion matrix :\\n{}\".format(metrics[\"Confusion matrix\"]))\n",
    "    print(\"\")\n",
    "    print(\"MS: {:.4f}\".format(metrics[\"MS\"]))\n",
    "    print(\"\")\n",
    "    print(\"QWK: {:.4f}\".format(metrics[\"QWK\"]))\n",
    "    print(\"\")\n",
    "    print(\"MAE: {:.4f}\".format(metrics[\"MAE\"]))\n",
    "    print(\"\")\n",
    "    print(\"CCR: {:.4f}\".format(metrics[\"CCR\"]))\n",
    "    print(\"\")\n",
    "    print(\"1-off: {:.4f}\".format(metrics[\"1-off\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: OBDECOCModel,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    H: dict,\n",
    "    num_classes: int,\n",
    "):  # H: dict\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    progress_bar = tqdm(total=num_batches, ncols=100, position=0, desc=\"Train progress\")\n",
    "    model.train()\n",
    "    mean_loss, accuracy = 0, 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)  # Inputs and labels to device\n",
    "\n",
    "        # Compute prediction error and accuracy of the training process\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        mean_loss += loss\n",
    "        accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stack predictions and true labels to determine the confusion matrix\n",
    "        pred_np = model.transformer.labels(pred).cpu().detach().numpy()\n",
    "        true_np = y.cpu().detach().numpy()\n",
    "        if y_pred is None:\n",
    "            y_pred = pred_np\n",
    "        else:\n",
    "            y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "        if y_true is None:\n",
    "            y_true = true_np\n",
    "        else:\n",
    "            y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    accuracy /= size\n",
    "    mean_loss /= num_batches\n",
    "\n",
    "    H[\"train_loss\"].append(loss.cpu().detach().numpy())\n",
    "    H[\"train_acc\"].append(accuracy)\n",
    "\n",
    "    # Confusion matrix for training\n",
    "    labels = range(0, num_classes)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print(\"\")\n",
    "    print(\"Train Confusion matrix :\\n{}\".format(conf_mat))\n",
    "    print(\"\")\n",
    "\n",
    "    return accuracy, mean_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    model: OBDECOCModel,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    num_classes: int,\n",
    "):\n",
    "    num_batches = len(test_dataloader)\n",
    "    progress_bar = tqdm(total=num_batches, ncols=100, position=0, desc=\"Test progress\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(test_dataloader):\n",
    "            X, y = X.to(device), y.to(device)  # inputs and labels to device\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # Stack predictions and true labels\n",
    "            pred_np = model.transformer.labels(pred).cpu().detach().numpy()\n",
    "            true_np = y.cpu().detach().numpy()\n",
    "            if y_pred is None:\n",
    "                y_pred = pred_np\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "            if y_true is None:\n",
    "                y_true = true_np\n",
    "            else:\n",
    "                y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=test_loss / (batch + 1))\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    metrics = compute_metrics(y_true, y_pred, num_classes)\n",
    "    print_metrics(metrics)\n",
    "\n",
    "    return metrics, test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: OBDECOCModel,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    H: dict,\n",
    "    num_classes: int,\n",
    "):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0, 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y)\n",
    "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            pred_np = model.transformer.labels(pred).cpu().detach().numpy()\n",
    "            true_np = y.cpu().detach().numpy()\n",
    "            if y_pred is None:\n",
    "                y_pred = pred_np\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "            if y_true is None:\n",
    "                y_true = true_np\n",
    "            else:\n",
    "                y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "    accuracy /= size\n",
    "    loss /= num_batches\n",
    "\n",
    "    H[\"val_loss\"].append(loss.cpu().detach().numpy())\n",
    "    H[\"val_acc\"].append(accuracy)\n",
    "\n",
    "    metrics = compute_metrics(y_true, y_pred, num_classes)\n",
    "\n",
    "    return metrics, accuracy, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|█████████████████████████| 4/4 [00:11<00:00,  2.90s/it, accuracy=74, loss=39.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[  0  71   2   1   0   0]\n",
      " [  0 175  16  13   0   1]\n",
      " [  0  44  25  32   3   7]\n",
      " [  0  19  21  81   9  13]\n",
      " [  0  11   8  35   7  39]\n",
      " [  0   0   0   6   3  38]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/5\n",
      "Train loss: 86.180344, Train accuracy: 0.1088\n",
      "Val loss: 138.933105, Val accuracy: 0.1074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|█████████████████████████| 4/4 [00:10<00:00,  2.69s/it, accuracy=74, loss=31.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[  0  70   2   2   0   0]\n",
      " [  0 181  16   8   0   0]\n",
      " [  0  44  25  37   1   4]\n",
      " [  0  14  17  99   3  10]\n",
      " [  0   5   9  59  11  16]\n",
      " [  0   0   0   1   3  43]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/5\n",
      "Train loss: 73.077477, Train accuracy: 0.1088\n",
      "Val loss: 124.414589, Val accuracy: 0.1074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|███████████████████████████| 4/4 [00:10<00:00,  2.74s/it, accuracy=74, loss=22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[  0  73   0   1   0   0]\n",
      " [  0 192  10   3   0   0]\n",
      " [  0  42  36  32   1   0]\n",
      " [  0  12  20  96  13   2]\n",
      " [  0   3   7  44  27  19]\n",
      " [  0   0   0   0   2  45]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/5\n",
      "Train loss: 59.367184, Train accuracy: 0.1088\n",
      "Val loss: 112.904716, Val accuracy: 0.1074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|█████████████████████████| 4/4 [00:11<00:00,  2.78s/it, accuracy=74, loss=28.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[  0  72   0   2   0   0]\n",
      " [  0 195   7   3   0   0]\n",
      " [  0  37  43  29   2   0]\n",
      " [  0   4  14 113   9   3]\n",
      " [  0   1   7  41  33  18]\n",
      " [  0   0   0   0   4  43]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/5\n",
      "Train loss: 51.117718, Train accuracy: 0.1088\n",
      "Val loss: 101.604919, Val accuracy: 0.1074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|█████████████████████████| 4/4 [00:11<00:00,  2.78s/it, accuracy=74, loss=22.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[  0  74   0   0   0   0]\n",
      " [  0 196   7   2   0   0]\n",
      " [  0  23  49  39   0   0]\n",
      " [  0   3   9 131   0   0]\n",
      " [  0   1   1  49  31  18]\n",
      " [  0   0   0   0   0  47]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 5/5\n",
      "Train loss: 44.203930, Train accuracy: 0.1088\n",
      "Val loss: 114.216644, Val accuracy: 0.1074\n",
      "\n",
      "[INFO] Network evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test progress: 100%|█████████████████████████████████████████| 2/2 [00:01<00:00,  1.37it/s, loss=88]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix :\n",
      "[[ 0 14  5  3  0  0]\n",
      " [ 0 27 16 14  2  1]\n",
      " [ 0 11  7 13  2  0]\n",
      " [ 0  5 11 22  2  2]\n",
      " [ 0  0  3 21  3  3]\n",
      " [ 0  1  2  3  3  5]]\n",
      "\n",
      "MS: 0.0000\n",
      "\n",
      "QWK: 0.5270\n",
      "\n",
      "MAE: 0.9502\n",
      "\n",
      "CCR: 0.3184\n",
      "\n",
      "1-off: 0.7861\n",
      "\n",
      "[INFO] Total training time: 61.98s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "H = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# To store validation metrics\n",
    "validation_metrics = {}\n",
    "\n",
    "# Definition to store best model weights\n",
    "best_model_weights = model.state_dict()\n",
    "best_qwk = 0.0\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(optimiser_params[\"epochs\"]):\n",
    "    train_acc, train_loss = train(\n",
    "        train_dataloader, model, loss_fn, optimizer, device, H, num_classes=num_classes\n",
    "    )\n",
    "    validation_metrics, val_acc, val_loss = validate(\n",
    "        val_dataloader, model, loss_fn, device, H, num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    if validation_metrics[\"QWK\"] >= best_qwk:\n",
    "        best_qwk = validation_metrics[\"QWK\"]\n",
    "        best_model_weights = deepcopy(model.state_dict())\n",
    "\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, optimiser_params[\"epochs\"]))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(val_loss, val_acc))\n",
    "\n",
    "# Store last train error\n",
    "train_error = H[\"train_loss\"][-1]\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_model_weights)\n",
    "\n",
    "# Start evaluation\n",
    "print(\"[INFO] Network evaluation ...\")\n",
    "\n",
    "test_metrics, test_loss = test(\n",
    "    test_dataloader, model, loss_fn, device, num_classes=num_classes\n",
    ")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(\"\\n[INFO] Total training time: {:.2f}s\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385611db6ca4af2663855b1744f455946eef985f7b33eb977c97667790417df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
