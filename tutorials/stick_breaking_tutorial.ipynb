{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from dlordinal.output_layers import StickBreakingLayer\n",
    "from dlordinal.losses import TriangularCrossEntropyLoss\n",
    "from dlordinal.datasets import FGNet\n",
    "from sklearn.metrics import (accuracy_score, cohen_kappa_score,\n",
    "                             confusion_matrix, mean_absolute_error)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "from torch import cuda\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess of FGNet dataset\n",
    "\n",
    "First, we present the configuration parameters for the experimentation and the number of workers for the *DataLoader*, which defines the number of subprocesses to use for data loading. In this specific case, it refers to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_params = {\n",
    "    'lr': 1e-3,\n",
    "    'bs': 200,\n",
    "    'epochs': 5,\n",
    "    's': 2,\n",
    "    'c': 0.2,\n",
    "    'beta': 0.5\n",
    "}\n",
    "\n",
    "workers = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the *FGNet* method to download and preprocess the images. Once that is done with the training data, we create a validation partition comprising 15% of the data using the *StratifiedShuffleSplit* method. Finally, with all the partitions, we load the images using a method called *DataLoader*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Using cpu device\n",
      "Detected image shape: [3, 128, 128]\n",
      "class_weights=array([1.01908397, 1.53448276, 0.79464286, 1.13135593, 0.55165289,\n",
      "       2.42727273])\n"
     ]
    }
   ],
   "source": [
    "fgnet_trainval = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "test_data = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "num_classes = len(fgnet_trainval.classes)\n",
    "classes = fgnet_trainval.classes\n",
    "targets = fgnet_trainval.targets\n",
    "\n",
    "# Create a validation split\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=0)\n",
    "sss_splits = list(\n",
    "    sss.split(X=np.zeros(len(fgnet_trainval)), y=fgnet_trainval.targets)\n",
    ")\n",
    "train_idx, val_idx = sss_splits[0]\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_data = Subset(fgnet_trainval, train_idx)\n",
    "val_data = Subset(fgnet_trainval, val_idx)\n",
    "\n",
    "# Get CUDA device\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=optimiser_params[\"bs\"], shuffle=True, num_workers=workers\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_data, batch_size=optimiser_params[\"bs\"], shuffle=True, num_workers=workers\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=optimiser_params[\"bs\"], shuffle=False, num_workers=workers\n",
    ")\n",
    "\n",
    "# Get image shape\n",
    "img_shape = None\n",
    "for X, _ in train_dataloader:\n",
    "    img_shape = list(X.shape[1:])\n",
    "    break\n",
    "print(f\"Detected image shape: {img_shape}\")\n",
    "\n",
    "# Define class weights for imbalanced datasets\n",
    "classes_array = np.array([int(c) for c in classes])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=classes_array, y=targets\n",
    ")\n",
    "print(f\"{class_weights=}\")\n",
    "class_weights = (\n",
    "    torch.from_numpy(class_weights).float().to(device)\n",
    ")  # Transform to Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We are using a pretrained *ResNet* model, which has previously been trained on ImageNet. We are modifying the last fully connected layer by a methodology called *Stick Breaking*.\n",
    "\n",
    "The stick breaking approach considers the problem of breaking a stick of length 1 into J segments. This methodology is related to non-parametric Bayesian methods and can be considered a subset of the random allocation processes [1].\n",
    "\n",
    "Finally, we define the *Adam* optimiser, which is used to adjust the network's weights and minimize the error of a loss function.\n",
    "\n",
    "[1]: Vargas, Víctor Manuel et al. (2022). *Unimodal regularisation based on beta distribution for deep ordinal regression.* Pattern Recognition, 122, 108310. Elsevier. doi.org/10.1016/j.patcog.2021.108310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model.fc = StickBreakingLayer(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = Adam(model.parameters(), lr=optimiser_params[\"lr\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x; a_j, b_j, c_j) &= \\begin{cases}\n",
    "   0, & x < a_j, \\\\\n",
    "   \\frac{2(x - a_j)}{(b_j - a_j)(c_j - a_j)}, & a_j \\leq x < c_j, \\\\\n",
    "   \\frac{2(b_j - x)}{(b_j - a_j)(b_j - c_j)}, & c_j \\leq x < b_j, \\\\\n",
    "   0, & b_j \\leq x,\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "The triangular distribution [1] can be determined using three parameters a, b and c, which define the lower limit, upper limit, and mode, respectively. These parameters also determine the x coordinate of each of the vertices of the triangle.\n",
    "\n",
    "The distributions employed for the extreme classes should differ from those utilized for the intermediate ones. Consequently, the distributions for the initial and final classes should allocate their probabilities just in one direction: positively for the first class and negatively for the last one.\n",
    "\n",
    "[1]: Víctor Manuel Vargas, Pedro Antonio Gutiérrez, Javier Barbero-Gómez, and César Hervás-Martínez (2023). *Soft Labelling Based on Triangular Distributions for Ordinal Classification.* Information Fusion, 93, 258--267. doi.org/10.1016/j.inffus.2023.01.003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = TriangularCrossEntropyLoss(num_classes=num_classes).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics computation\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, \n",
    "    y_pred: np.ndarray, \n",
    "    num_classes: int):\n",
    "\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    labels = range(0, num_classes)\n",
    "\n",
    "    # Metrics calculation\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic', labels=labels)\n",
    "    ms = minimum_sensitivity(y_true, y_pred, labels=labels)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    off1 = accuracy_off1(y_true, y_pred, labels=labels)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    metrics = {\n",
    "        'QWK': qwk,\n",
    "        'MS': ms,\n",
    "        'MAE': mae,\n",
    "        'CCR': acc,\n",
    "        '1-off': off1,\n",
    "        'Confusion matrix': conf_mat\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _compute_sensitivities(y_true, y_pred, labels=None):\n",
    "\tif len(y_true.shape) > 1:\n",
    "\t\ty_true = np.argmax(y_true, axis=1)\n",
    "\tif len(y_pred.shape) > 1:\n",
    "\t\ty_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\tconf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "\tsum = np.sum(conf_mat, axis=1)\n",
    "\tmask = np.eye(conf_mat.shape[0], conf_mat.shape[1])\n",
    "\tcorrect = np.sum(conf_mat * mask, axis=1)\n",
    "\tsensitivities = correct / sum\n",
    "\n",
    "\tsensitivities = sensitivities[~np.isnan(sensitivities)]\n",
    "\n",
    "\treturn sensitivities\n",
    "\n",
    "\n",
    "def minimum_sensitivity(y_true, y_pred, labels=None):\n",
    "\treturn np.min(_compute_sensitivities(y_true, y_pred, labels=labels))\n",
    "\n",
    "\n",
    "def accuracy_off1(y_true, y_pred, labels=None):\n",
    "\tif len(y_true.shape) > 1:\n",
    "\t\ty_true = np.argmax(y_true, axis=1)\n",
    "\tif len(y_pred.shape) > 1:\n",
    "\t\ty_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\tconf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\tn = conf_mat.shape[0]\n",
    "\tmask = np.eye(n, n) + np.eye(n, n, k=1), + np.eye(n, n, k=-1)\n",
    "\tcorrect = mask * conf_mat\n",
    "\n",
    "\treturn 1.0 * np.sum(correct) / np.sum(conf_mat)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(\"\")\n",
    "    print('Confusion matrix :\\n{}'.format(metrics['Confusion matrix']))\n",
    "    print(\"\")\n",
    "    print('MS: {:.4f}'.format(metrics['MS']))\n",
    "    print(\"\")\n",
    "    print('QWK: {:.4f}'.format(metrics['QWK']))\n",
    "    print(\"\")\n",
    "    print('MAE: {:.4f}'.format(metrics['MAE']))\n",
    "    print(\"\")\n",
    "    print('CCR: {:.4f}'.format(metrics['CCR']))\n",
    "    print(\"\")\n",
    "    print('1-off: {:.4f}'.format(metrics['1-off']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    H: dict,\n",
    "    num_classes: int,\n",
    "):  # H: dict\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    progress_bar = tqdm(total=num_batches, ncols=100, position=0, desc=\"Train progress\")\n",
    "    model.train()\n",
    "    mean_loss, accuracy = 0, 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)  # Inputs and labels to device\n",
    "\n",
    "        # Compute prediction error and accuracy of the training process\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        mean_loss += loss\n",
    "        accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stack predictions and true labels to determine the confusion matrix\n",
    "        pred_np = pred.argmax(1).cpu().detach().numpy()\n",
    "        true_np = y.cpu().detach().numpy()\n",
    "        if y_pred is None:\n",
    "            y_pred = pred_np\n",
    "        else:\n",
    "            y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "        if y_true is None:\n",
    "            y_true = true_np\n",
    "        else:\n",
    "            y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    accuracy /= size\n",
    "    mean_loss /= num_batches\n",
    "\n",
    "    H[\"train_loss\"].append(loss.cpu().detach().numpy())\n",
    "    H[\"train_acc\"].append(accuracy)\n",
    "\n",
    "    # Confusion matrix for training\n",
    "    labels = range(0, num_classes)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print(\"\")\n",
    "    print(\"Train Confusion matrix :\\n{}\".format(conf_mat))\n",
    "    print(\"\")\n",
    "\n",
    "    return accuracy, mean_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    num_classes: int,\n",
    "):\n",
    "    num_batches = len(test_dataloader)\n",
    "    progress_bar = tqdm(total=num_batches, ncols=100, position=0, desc=\"Test progress\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(test_dataloader):\n",
    "            X, y = X.to(device), y.to(device)  # inputs and labels to device\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # Stack predictions and true labels\n",
    "            pred_np = pred.argmax(1).cpu().detach().numpy()\n",
    "            true_np = y.cpu().detach().numpy()\n",
    "            if y_pred is None:\n",
    "                y_pred = pred_np\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "            if y_true is None:\n",
    "                y_true = true_np\n",
    "            else:\n",
    "                y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=test_loss / (batch + 1))\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    metrics = compute_metrics(y_true, y_pred, num_classes)\n",
    "    print_metrics(metrics)\n",
    "\n",
    "    return metrics, test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    H: dict,\n",
    "    num_classes: int,\n",
    "):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0, 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y)\n",
    "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            pred_np = pred.argmax(1).cpu().detach().numpy()\n",
    "            true_np = y.cpu().detach().numpy()\n",
    "            if y_pred is None:\n",
    "                y_pred = pred_np\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "            if y_true is None:\n",
    "                y_true = true_np\n",
    "            else:\n",
    "                y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "    accuracy /= size\n",
    "    loss /= num_batches\n",
    "\n",
    "    H[\"val_loss\"].append(loss.cpu().detach().numpy())\n",
    "    H[\"val_acc\"].append(accuracy)\n",
    "\n",
    "    metrics = compute_metrics(y_true, y_pred, num_classes)\n",
    "\n",
    "    return metrics, accuracy, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|████████████████████████| 4/4 [00:11<00:00,  2.81s/it, accuracy=118, loss=1.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 64   4   5   0   1   0]\n",
      " [143  20  20  13   9   0]\n",
      " [ 73  12   9  11   6   0]\n",
      " [ 92  12  12  10  16   1]\n",
      " [ 56   8   8  15  12   1]\n",
      " [ 29   3   1   4   7   3]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/5\n",
      "Train loss: 1.847242, Train accuracy: 0.1735\n",
      "Val loss: 2.574011, Val accuracy: 0.1074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|████████████████████████| 4/4 [00:09<00:00,  2.28s/it, accuracy=379, loss=1.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 60  12   0   2   0   0]\n",
      " [ 26 109  22  31  16   1]\n",
      " [  4  14  18  55  20   0]\n",
      " [  7   4   1 106  24   1]\n",
      " [  2   0   0  32  63   3]\n",
      " [  0   1   1   7  15  23]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/5\n",
      "Train loss: 1.367832, Train accuracy: 0.5574\n",
      "Val loss: 2.786200, Val accuracy: 0.1074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|█████████████████████████| 4/4 [00:09<00:00,  2.41s/it, accuracy=422, loss=1.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 60  14   0   0   0   0]\n",
      " [ 23 162   3  11   5   1]\n",
      " [  9  44   7  36  13   2]\n",
      " [  7  17   0 105  14   0]\n",
      " [  3   9   0  17  51  20]\n",
      " [  2   0   0   1   7  37]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/5\n",
      "Train loss: 1.251675, Train accuracy: 0.6206\n",
      "Val loss: 2.030842, Val accuracy: 0.1901\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|████████████████████████| 4/4 [00:09<00:00,  2.40s/it, accuracy=479, loss=1.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 63  11   0   0   0   0]\n",
      " [ 13 174   7   9   2   0]\n",
      " [  6  31  28  38   8   0]\n",
      " [  4   2  13 118   5   1]\n",
      " [  7   4   0  18  60  11]\n",
      " [  0   2   0   0   9  36]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/5\n",
      "Train loss: 1.168523, Train accuracy: 0.7044\n",
      "Val loss: 1.750762, Val accuracy: 0.2562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|████████████████████████| 4/4 [00:08<00:00,  2.23s/it, accuracy=534, loss=1.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 69   5   0   0   0   0]\n",
      " [ 15 178   7   4   1   0]\n",
      " [ 10  27  42  27   5   0]\n",
      " [  1   1  11 124   6   0]\n",
      " [  2   3   2   8  82   3]\n",
      " [  0   0   0   0   8  39]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 5/5\n",
      "Train loss: 1.091701, Train accuracy: 0.7853\n",
      "Val loss: 1.488901, Val accuracy: 0.3554\n",
      "\n",
      "[INFO] Network evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test progress: 100%|███████████████████████████████████████| 2/2 [00:01<00:00,  1.36it/s, loss=1.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix :\n",
      "[[22  0  0  0  0  0]\n",
      " [33 22  4  1  0  0]\n",
      " [ 5 16  4  6  2  0]\n",
      " [ 8  9  3 14  6  2]\n",
      " [ 2  1  3  9 12  3]\n",
      " [ 2  0  0  0  8  4]]\n",
      "\n",
      "MS: 0.1212\n",
      "\n",
      "QWK: 0.6697\n",
      "\n",
      "MAE: 0.8806\n",
      "\n",
      "CCR: 0.3881\n",
      "\n",
      "1-off: 0.8259\n",
      "\n",
      "[INFO] Total training time: 55.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "H = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# To store validation metrics\n",
    "validation_metrics = {}\n",
    "\n",
    "# Definition to store best model weights\n",
    "best_model_weights = model.state_dict()\n",
    "best_qwk = 0.0\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(optimiser_params[\"epochs\"]):\n",
    "    train_acc, train_loss = train(\n",
    "        train_dataloader, model, loss_fn, optimizer, device, H, num_classes=num_classes\n",
    "    )\n",
    "    validation_metrics, val_acc, val_loss = validate(\n",
    "        val_dataloader, model, loss_fn, device, H, num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    if validation_metrics[\"QWK\"] >= best_qwk:\n",
    "        best_qwk = validation_metrics[\"QWK\"]\n",
    "        best_model_weights = deepcopy(model.state_dict())\n",
    "\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, optimiser_params[\"epochs\"]))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(val_loss, val_acc))\n",
    "\n",
    "# Store last train error\n",
    "train_error = H[\"train_loss\"][-1]\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_model_weights)\n",
    "\n",
    "# Start evaluation\n",
    "print(\"[INFO] Network evaluation ...\")\n",
    "\n",
    "test_metrics, test_loss = test(\n",
    "    test_dataloader, model, loss_fn, device, num_classes=num_classes\n",
    ")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(\"\\n[INFO] Total training time: {:.2f}s\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385611db6ca4af2663855b1744f455946eef985f7b33eb977c97667790417df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
