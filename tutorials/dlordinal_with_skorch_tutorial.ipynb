{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch import cuda, nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "from dlordinal.datasets import FGNet\n",
    "from dlordinal.losses import TriangularCrossEntropyLoss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess of FGNet dataset\n",
    "\n",
    "First, we present the configuration parameters for the experimentation and the number of workers for the `DataLoader`, which defines the number of subprocesses to use for data loading. In this specific case, it refers to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_params = {\"lr\": 1e-3, \"bs\": 400, \"epochs\": 5, \"s\": 2, \"c\": 0.2, \"beta\": 0.5}\n",
    "\n",
    "workers = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `FGNet` method to download and preprocess the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "fgnet_train = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "fgnet_test = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "num_classes = len(fgnet_train.classes)\n",
    "classes = fgnet_train.classes\n",
    "targets = fgnet_train.targets\n",
    "\n",
    "# Get CUDA device\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator\n",
    "\n",
    "We are setting up a deep learning model using `PyTorch` and `Skorch`. First, we define the model architecture using ResNet18, a pre-trained convolutional neural network, and customize its fully connected layer to match the number of classes in our classification task. Then we specify the loss function, in this case, a custom Triangular Cross Entropy Loss[1]. Finally, we configure the Skorch estimator, which serves as a bridge between PyTorch and scikit-learn, allowing us to train and evaluate our model seamlessly. We provide the model, loss function, and optimiser details such as the learning rate and number of epochs to the estimator. Additionally, we specify parameters for data loading and processing, like batch size and the number of workers, to optimise training performance.\n",
    "\n",
    "[1]: Víctor Manuel Vargas, Pedro Antonio Gutiérrez, Javier Barbero-Gómez, and César Hervás-Martínez (2023). *Soft Labelling Based on Triangular Distributions for Ordinal Classification.* Information Fusion, 93, 258--267. doi.org/10.1016/j.inffus.2023.01.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = TriangularCrossEntropyLoss(num_classes=num_classes).to(device)\n",
    "\n",
    "# Skorch estimator\n",
    "estimator = NeuralNetClassifier(\n",
    "    module=model,\n",
    "    criterion=loss_fn,\n",
    "    optimizer=Adam,\n",
    "    lr=optimiser_params[\"lr\"],\n",
    "    max_epochs=optimiser_params[\"epochs\"],\n",
    "    train_split=None,\n",
    "    callbacks=[],\n",
    "    device=device,\n",
    "    verbose=0,\n",
    "    iterator_train__batch_size=optimiser_params[\"bs\"],\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__num_workers=workers - 1,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_valid__batch_size=optimiser_params[\"bs\"],\n",
    "    iterator_valid__shuffle=False,\n",
    "    iterator_valid__num_workers=workers - 1,\n",
    "    iterator_valid__pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=6, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(X=fgnet_train, y=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train probabilities = train_probs=array([[ 2.9335966e+00,  1.6396730e+00, -4.1097212e+00,  4.2018917e-01,\n",
      "        -1.4565204e+00, -4.3641205e+00],\n",
      "       [ 8.1791782e+00,  2.8562646e+00, -9.0971680e+00,  1.6580626e-02,\n",
      "        -3.8278933e+00, -7.0976620e+00],\n",
      "       [ 9.5499592e+00,  3.5849128e+00, -8.4043884e+00, -4.2262230e-02,\n",
      "        -5.7516675e+00, -7.0723133e+00],\n",
      "       ...,\n",
      "       [ 1.7538257e+00,  1.8087029e-04, -6.2550454e+00,  2.6264958e+00,\n",
      "         3.1415778e-01, -4.3745127e+00],\n",
      "       [-6.1797386e-01, -6.9311045e-02, -3.2306197e+00,  1.5903845e+00,\n",
      "        -5.8013773e-01, -1.2073216e+00],\n",
      "       [-1.2833995e+00, -2.5462928e-01, -3.4825776e+00,  2.6367762e+00,\n",
      "         2.4873786e-01, -1.8333929e+00]], dtype=float32)\n",
      "\n",
      "Test probabilities = test_probs=array([[-0.30555367, -0.06686927, -2.9128444 ,  2.4259052 ,  0.38098484,\n",
      "        -2.2845333 ],\n",
      "       [ 6.7591906 ,  4.8296413 , -4.5853643 ,  0.25159836, -5.5993004 ,\n",
      "        -6.9446783 ],\n",
      "       [ 1.775908  ,  2.4953337 , -3.323121  ,  1.694654  , -1.8339258 ,\n",
      "        -4.3164835 ],\n",
      "       ...,\n",
      "       [-1.7040929 , -1.338182  , -4.237404  ,  3.5072465 ,  1.2669804 ,\n",
      "        -2.8282924 ],\n",
      "       [ 1.7215905 ,  1.0412043 , -3.7369957 ,  1.7478234 , -0.50562465,\n",
      "        -3.7393394 ],\n",
      "       [-1.3184199 ,  0.42436934, -1.2091427 ,  1.731024  ,  0.15624674,\n",
      "        -2.6779366 ]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_probs = estimator.predict_proba(fgnet_train)\n",
    "print(f\"Train probabilities = {train_probs=}\\n\")\n",
    "\n",
    "test_probs = estimator.predict_proba(fgnet_test)\n",
    "print(f\"Test probabilities = {test_probs=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385611db6ca4af2663855b1744f455946eef985f7b33eb977c97667790417df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
