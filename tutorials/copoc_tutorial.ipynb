{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2702f22b2d8b589",
   "metadata": {},
   "source": "## Import libraries"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T10:15:07.182879Z",
     "start_time": "2025-09-29T10:14:54.153191Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch import cuda, nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "from torchvision.transforms.v2 import Compose, ToDtype, ToImage\n",
    "\n",
    "from dlordinal.datasets import FGNet\n",
    "from dlordinal.metrics import accuracy_off1, amae, mmae, ranked_probability_score\n",
    "from dlordinal.output_layers.copoc import COPOC"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "f56c97d9a3dbf3ff",
   "metadata": {},
   "source": "## Import FGNet dataset"
  },
  {
   "cell_type": "code",
   "id": "b63b68abab6465cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T10:15:07.309925Z",
     "start_time": "2025-09-29T10:15:07.218008Z"
    }
   },
   "source": [
    "fgnet_train = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=Compose([ToImage(), ToDtype(torch.float32, scale=True)]),\n",
    ")\n",
    "\n",
    "fgnet_test = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=Compose([ToImage(), ToDtype(torch.float32, scale=True)]),\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "166d6a8141678f1b",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Fine-tuning of Resnet18 on the FGNet dataset using the Conformal Predictions for OC (COPOC) output layer(s) from\n",
    "\n",
    "Dey, P., Merugu, S., & Kaveri, S. R. (2023). Conformal prediction sets for ordinal classification. Advances in Neural Information Processing Systems, 36, 879-899,\n",
    "\n",
    "which enforce unimodality in the output probabilities in a non-parametric way."
   ]
  },
  {
   "cell_type": "code",
   "id": "848ea2b21d4c3708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T10:27:56.482700Z",
     "start_time": "2025-09-29T10:15:07.341201Z"
    }
   },
   "source": [
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_classes = len(fgnet_train.classes)\n",
    "\n",
    "# Initialize ResNet18 model\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Add COPOC layer\n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, num_classes), COPOC())\n",
    "model = model.to(device)\n",
    "\n",
    "# Skorch estimator\n",
    "estimator = NeuralNetClassifier(\n",
    "    module=model,\n",
    "    criterion=CrossEntropyLoss().to(device),\n",
    "    optimizer=Adam,\n",
    "    lr=0.001,\n",
    "    max_epochs=30,\n",
    "    device=device,\n",
    "    batch_size=200,\n",
    ")\n",
    "\n",
    "# Prepare training labels\n",
    "y_train = torch.tensor(fgnet_train.targets, dtype=torch.long)\n",
    "\n",
    "# Train model\n",
    "estimator.fit(fgnet_train, y_train)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001B[36m1.9458\u001B[0m       \u001B[32m0.1056\u001B[0m        \u001B[35m2.9033\u001B[0m  27.3399\n",
      "      2        \u001B[36m1.7937\u001B[0m       \u001B[32m0.1988\u001B[0m        4.4783  25.9128\n",
      "      3        \u001B[36m1.7733\u001B[0m       0.1056       14.8507  25.8848\n",
      "      4        \u001B[36m1.6589\u001B[0m       0.0994        9.0877  25.1364\n",
      "      5        \u001B[36m1.4085\u001B[0m       0.1429        3.2452  22.2829\n",
      "      6        \u001B[36m1.3573\u001B[0m       0.1988        \u001B[35m2.3565\u001B[0m  25.1594\n",
      "      7        \u001B[36m1.2720\u001B[0m       \u001B[32m0.2174\u001B[0m        \u001B[35m2.0465\u001B[0m  49.2505\n",
      "      8        \u001B[36m1.1944\u001B[0m       \u001B[32m0.2547\u001B[0m        \u001B[35m1.9980\u001B[0m  51.6799\n",
      "      9        \u001B[36m1.0869\u001B[0m       \u001B[32m0.2919\u001B[0m        \u001B[35m1.4406\u001B[0m  51.9515\n",
      "     10        \u001B[36m1.0338\u001B[0m       \u001B[32m0.3043\u001B[0m        1.4702  20.6470\n",
      "     11        \u001B[36m0.9513\u001B[0m       \u001B[32m0.3168\u001B[0m        \u001B[35m1.3803\u001B[0m  22.8812\n",
      "     12        \u001B[36m0.9263\u001B[0m       0.2981        1.4277  21.5922\n",
      "     13        \u001B[36m0.8541\u001B[0m       \u001B[32m0.4037\u001B[0m        \u001B[35m1.3414\u001B[0m  21.4255\n",
      "     14        \u001B[36m0.7964\u001B[0m       \u001B[32m0.4224\u001B[0m        1.3526  21.9901\n",
      "     15        \u001B[36m0.7301\u001B[0m       0.3851        1.3938  21.1809\n",
      "     16        \u001B[36m0.6904\u001B[0m       \u001B[32m0.5031\u001B[0m        \u001B[35m1.2744\u001B[0m  29.1702\n",
      "     17        \u001B[36m0.5542\u001B[0m       0.3789        1.5669  21.7662\n",
      "     18        \u001B[36m0.5222\u001B[0m       \u001B[32m0.5093\u001B[0m        1.4713  21.4015\n",
      "     19        \u001B[36m0.4454\u001B[0m       0.4845        1.5930  21.8219\n",
      "     20        \u001B[36m0.4265\u001B[0m       0.3292        1.8480  22.1104\n",
      "     21        \u001B[36m0.3509\u001B[0m       0.3602        1.7891  21.7342\n",
      "     22        \u001B[36m0.2528\u001B[0m       0.4845        1.4015  21.1203\n",
      "     23        \u001B[36m0.1866\u001B[0m       0.5093        1.3674  21.8923\n",
      "     24        \u001B[36m0.1380\u001B[0m       0.4658        1.6375  21.3231\n",
      "     25        \u001B[36m0.1011\u001B[0m       \u001B[32m0.5342\u001B[0m        1.4541  22.2834\n",
      "     26        \u001B[36m0.0918\u001B[0m       0.5031        1.5183  22.2268\n",
      "     27        \u001B[36m0.0661\u001B[0m       0.5031        1.6007  21.5487\n",
      "     28        \u001B[36m0.0552\u001B[0m       \u001B[32m0.5590\u001B[0m        1.4888  21.8936\n",
      "     29        \u001B[36m0.0378\u001B[0m       0.5466        1.5707  22.0572\n",
      "     30        \u001B[36m0.0235\u001B[0m       0.4907        1.5522  21.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=6, bias=True)\n",
       "      (1): COPOC()\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "73ca28be97f240d4",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Focus of the evaluation is on checking the unimodality of the predictive probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "id": "29b8798ece302022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T10:27:59.526949Z",
     "start_time": "2025-09-29T10:27:56.531258Z"
    }
   },
   "source": [
    "def is_unimodal(probs):\n",
    "    \"\"\"Check if a 1D array is unimodal (increases to a peak, then decreases).\"\"\"\n",
    "    peak_idx = np.argmax(probs)\n",
    "    # Increasing up to peak\n",
    "    inc = np.all(np.diff(probs[: peak_idx + 1]) >= 0)\n",
    "    # Decreasing after peak\n",
    "    dec = np.all(np.diff(probs[peak_idx:]) <= 0)\n",
    "    return inc and dec\n",
    "\n",
    "\n",
    "def check_unimodality(y_pred):\n",
    "    \"\"\"Check unimodality for each row in y_pred and return the proportion.\"\"\"\n",
    "    unimodal_flags = np.array([is_unimodal(row) for row in y_pred])\n",
    "    # Proportion of rows that are unimodal\n",
    "    proportion = np.mean(unimodal_flags)\n",
    "    print(\n",
    "        f\"Unimodal predictions: {np.sum(unimodal_flags)} / {len(y_pred)} ({proportion})\"\n",
    "    )\n",
    "    return proportion\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various metrics given true labels and predicted probabilities.\"\"\"\n",
    "    if np.allclose(np.sum(y_pred, axis=1), 1):\n",
    "        y_pred_proba = y_pred\n",
    "    else:\n",
    "        y_pred_proba = softmax(y_pred, axis=1)\n",
    "\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Metrics\n",
    "    amae_metric = amae(y_true, y_pred)\n",
    "    mmae_metric = mmae(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred_max)\n",
    "    acc = accuracy_score(y_true, y_pred_max)\n",
    "    acc_1off = accuracy_off1(y_true, y_pred)\n",
    "    qwk = cohen_kappa_score(y_true, y_pred_max, weights=\"quadratic\")\n",
    "    rps = ranked_probability_score(y_true, y_pred_proba)\n",
    "    # Check unimodality\n",
    "    unimodal_prop = check_unimodality(y_pred_proba)\n",
    "\n",
    "    metrics = {\n",
    "        \"ACC\": acc,\n",
    "        \"1OFF\": acc_1off,\n",
    "        \"MAE\": mae,\n",
    "        \"QWK\": qwk,\n",
    "        \"AMAE\": amae_metric,\n",
    "        \"MMAE\": mmae_metric,\n",
    "        \"RPS\": rps,\n",
    "        \"Unimodality\": unimodal_prop,\n",
    "    }\n",
    "\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred_max))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "test_probs = estimator.predict_proba(fgnet_test)\n",
    "print(calculate_metrics(fgnet_test.targets, test_probs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unimodal predictions: 201 / 201 (1.0)\n",
      "ACC: 0.46766169154228854\n",
      "1OFF: 0.8805970149253731\n",
      "MAE: 0.6567164179104478\n",
      "QWK: 0.751150585385547\n",
      "AMAE: 0.6882034632034632\n",
      "MMAE: 1.0\n",
      "RPS: 0.5108151327300118\n",
      "Unimodality: 1.0\n",
      "[[ 7 15  0  0  0  0]\n",
      " [ 0 27 20 12  1  0]\n",
      " [ 0  3 11 15  4  0]\n",
      " [ 0  0  3 31  6  2]\n",
      " [ 0  0  3  9 16  2]\n",
      " [ 0  0  0  2 10  2]]\n",
      "{'ACC': 0.46766169154228854, '1OFF': 0.8805970149253731, 'MAE': 0.6567164179104478, 'QWK': 0.751150585385547, 'AMAE': 0.6882034632034632, 'MMAE': 1.0, 'RPS': 0.5108151327300118, 'Unimodality': 1.0}\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
