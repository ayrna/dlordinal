{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import class_weight\n",
    "from torch import cuda, nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dlordinal.datasets import FGNet\n",
    "from dlordinal.dropout import HybridDropout, HybridDropoutContainer\n",
    "from dlordinal.losses import TriangularLoss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess of FGNet dataset\n",
    "\n",
    "First, we present the configuration parameters for the experimentation and the number of workers for the *DataLoader*, which defines the number of subprocesses to use for data loading. In this specific case, it refers to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_params = {\"lr\": 1e-3, \"bs\": 200, \"epochs\": 5, \"s\": 2, \"c\": 0.2, \"beta\": 0.5}\n",
    "\n",
    "workers = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the *FGNet* method to download and preprocess the images. Once that is done with the training data, we create a validation partition comprising 15% of the data using the *StratifiedShuffleSplit* method. Finally, with all the partitions, we load the images using a method called *DataLoader*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected image shape: [3, 128, 128]\n",
      "class_weights=array([1.53448276, 0.55165289, 1.01908397, 0.79464286, 1.13135593,\n",
      "       2.42727273])\n"
     ]
    }
   ],
   "source": [
    "fgnet_trainval = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "test_data = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    target_transform=np.array,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "num_classes = len(fgnet_trainval.classes)\n",
    "classes = fgnet_trainval.classes\n",
    "targets = fgnet_trainval.targets\n",
    "\n",
    "# Create a validation split\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=0)\n",
    "sss_splits = list(sss.split(X=np.zeros(len(fgnet_trainval)), y=fgnet_trainval.targets))\n",
    "train_idx, val_idx = sss_splits[0]\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_data = Subset(fgnet_trainval, train_idx)\n",
    "val_data = Subset(fgnet_trainval, val_idx)\n",
    "\n",
    "# Get CUDA device\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=optimiser_params[\"bs\"], shuffle=True, num_workers=workers\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_data, batch_size=optimiser_params[\"bs\"], shuffle=True, num_workers=workers\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=optimiser_params[\"bs\"], shuffle=False, num_workers=workers\n",
    ")\n",
    "\n",
    "# Get image shape\n",
    "img_shape = None\n",
    "for X, _ in train_dataloader:\n",
    "    img_shape = list(X.shape[1:])\n",
    "    break\n",
    "print(f\"Detected image shape: {img_shape}\")\n",
    "\n",
    "# Define class weights for imbalanced datasets\n",
    "classes_array = np.array([int(c) for c in classes])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=classes_array, y=targets\n",
    ")\n",
    "print(f\"{class_weights=}\")\n",
    "class_weights = (\n",
    "    torch.from_numpy(class_weights).float().to(device)\n",
    ")  # Transform to Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We are using a pretrained *ResNet* model, which has previously been trained on ImageNet. A new layer named *HybridDropout()*[1] has been introduced into the model as a regularization method.\n",
    "\n",
    "*HybridDropout()* is a regularization technique that combines standard dropout with an ordinal approach, which considers the correlation between the activation values of neurons and the target labels. \n",
    "\n",
    "Finally, we define the *Adam* optimiser, which is used to adjust the network's weights and minimize the error of a loss function.\n",
    "\n",
    "[1]: Bérchez-Moreno, Francisco et al. (2024). *Fusion of standard and ordinal dropout techniques to regularise deep models*. Information Fusion, 106, 102299. doi:10.1016/j.inffus.2024.102299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    HybridDropout(),\n",
    "    nn.Linear(256, num_classes),\n",
    ")\n",
    "\n",
    "# Crear una instancia del contenedor HybridDropoutContainer\n",
    "model = HybridDropoutContainer(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = Adam(model.parameters(), lr=optimiser_params[\"lr\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x; a_j, b_j, c_j) &= \\begin{cases}\n",
    "   0, & x < a_j, \\\\\n",
    "   \\frac{2(x - a_j)}{(b_j - a_j)(c_j - a_j)}, & a_j \\leq x < c_j, \\\\\n",
    "   \\frac{2(b_j - x)}{(b_j - a_j)(b_j - c_j)}, & c_j \\leq x < b_j, \\\\\n",
    "   0, & b_j \\leq x,\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "The triangular distribution [1] can be determined using three parameters a, b and c, which define the lower limit, upper limit, and mode, respectively. These parameters also determine the x coordinate of each of the vertices of the triangle.\n",
    "\n",
    "The distributions employed for the extreme classes should differ from those utilized for the intermediate ones. Consequently, the distributions for the initial and final classes should allocate their probabilities just in one direction: positively for the first class and negatively for the last one.\n",
    "\n",
    "[1]: Víctor Manuel Vargas, Pedro Antonio Gutiérrez, Javier Barbero-Gómez, and César Hervás-Martínez (2023). *Soft Labelling Based on Triangular Distributions for Ordinal Classification.* Information Fusion, 93, 258--267. doi.org/10.1016/j.inffus.2023.01.003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = TriangularLoss(base_loss=nn.CrossEntropyLoss(), num_classes=num_classes).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics computation\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, num_classes: int):\n",
    "\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    labels = range(0, num_classes)\n",
    "\n",
    "    # Metrics calculation\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\", labels=labels)\n",
    "    ms = minimum_sensitivity(y_true, y_pred, labels=labels)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    off1 = accuracy_off1(y_true, y_pred, labels=labels)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"QWK\": qwk,\n",
    "        \"MS\": ms,\n",
    "        \"MAE\": mae,\n",
    "        \"CCR\": acc,\n",
    "        \"1-off\": off1,\n",
    "        \"Confusion matrix\": conf_mat,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _compute_sensitivities(y_true, y_pred, labels=None):\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    sum = np.sum(conf_mat, axis=1)\n",
    "    mask = np.eye(conf_mat.shape[0], conf_mat.shape[1])\n",
    "    correct = np.sum(conf_mat * mask, axis=1)\n",
    "    sensitivities = correct / sum\n",
    "\n",
    "    sensitivities = sensitivities[~np.isnan(sensitivities)]\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "\n",
    "def minimum_sensitivity(y_true, y_pred, labels=None):\n",
    "    return np.min(_compute_sensitivities(y_true, y_pred, labels=labels))\n",
    "\n",
    "\n",
    "def accuracy_off1(y_true, y_pred, labels=None):\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    n = conf_mat.shape[0]\n",
    "    mask = np.eye(n, n) + np.eye(n, n, k=1), +np.eye(n, n, k=-1)\n",
    "    correct = mask * conf_mat\n",
    "\n",
    "    return 1.0 * np.sum(correct) / np.sum(conf_mat)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(\"\")\n",
    "    print(\"Confusion matrix :\\n{}\".format(metrics[\"Confusion matrix\"]))\n",
    "    print(\"\")\n",
    "    print(\"MS: {:.4f}\".format(metrics[\"MS\"]))\n",
    "    print(\"\")\n",
    "    print(\"QWK: {:.4f}\".format(metrics[\"QWK\"]))\n",
    "    print(\"\")\n",
    "    print(\"MAE: {:.4f}\".format(metrics[\"MAE\"]))\n",
    "    print(\"\")\n",
    "    print(\"CCR: {:.4f}\".format(metrics[\"CCR\"]))\n",
    "    print(\"\")\n",
    "    print(\"1-off: {:.4f}\".format(metrics[\"1-off\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    H: dict,\n",
    "    num_classes: int,\n",
    "):  # H: dict\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    progress_bar = tqdm(total=num_batches, ncols=100, position=0, desc=\"Train progress\")\n",
    "    model.train()\n",
    "    mean_loss, accuracy = 0, 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)  # Inputs and labels to device\n",
    "        model.set_targets(y)\n",
    "\n",
    "        # Compute prediction error and accuracy of the training process\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        mean_loss += loss\n",
    "        accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stack predictions and true labels to determine the confusion matrix\n",
    "        pred_np = pred.argmax(1).cpu().detach().numpy()\n",
    "        true_np = y.cpu().detach().numpy()\n",
    "        if y_pred is None:\n",
    "            y_pred = pred_np\n",
    "        else:\n",
    "            y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "        if y_true is None:\n",
    "            y_true = true_np\n",
    "        else:\n",
    "            y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    accuracy /= size\n",
    "    mean_loss /= num_batches\n",
    "\n",
    "    H[\"train_loss\"].append(loss.cpu().detach().numpy())\n",
    "    H[\"train_acc\"].append(accuracy)\n",
    "\n",
    "    # Confusion matrix for training\n",
    "    labels = range(0, num_classes)\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print(\"\")\n",
    "    print(\"Train Confusion matrix :\\n{}\".format(conf_mat))\n",
    "    print(\"\")\n",
    "\n",
    "    return accuracy, mean_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    num_classes: int,\n",
    "):\n",
    "    num_batches = len(test_dataloader)\n",
    "    progress_bar = tqdm(total=num_batches, ncols=100, position=0, desc=\"Test progress\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(test_dataloader):\n",
    "            X, y = X.to(device), y.to(device)  # inputs and labels to device\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # Stack predictions and true labels\n",
    "            pred_np = pred.argmax(1).cpu().detach().numpy()\n",
    "            true_np = y.cpu().detach().numpy()\n",
    "            if y_pred is None:\n",
    "                y_pred = pred_np\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "            if y_true is None:\n",
    "                y_true = true_np\n",
    "            else:\n",
    "                y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=test_loss / (batch + 1))\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    metrics = compute_metrics(y_true, y_pred, num_classes)\n",
    "    print_metrics(metrics)\n",
    "\n",
    "    return metrics, test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    H: dict,\n",
    "    num_classes: int,\n",
    "):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0, 0\n",
    "    y_pred, y_true = None, None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y)\n",
    "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            pred_np = pred.argmax(1).cpu().detach().numpy()\n",
    "            true_np = y.cpu().detach().numpy()\n",
    "            if y_pred is None:\n",
    "                y_pred = pred_np\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, pred_np))\n",
    "\n",
    "            if y_true is None:\n",
    "                y_true = true_np\n",
    "            else:\n",
    "                y_true = np.concatenate((y_true, true_np))\n",
    "\n",
    "    accuracy /= size\n",
    "    loss /= num_batches\n",
    "\n",
    "    H[\"val_loss\"].append(loss.cpu().detach().numpy())\n",
    "    H[\"val_acc\"].append(accuracy)\n",
    "\n",
    "    metrics = compute_metrics(y_true, y_pred, num_classes)\n",
    "\n",
    "    return metrics, accuracy, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|████████████████████████| 4/4 [00:09<00:00,  2.40s/it, accuracy=200, loss=1.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 11  41   4   5   2  11]\n",
      " [  3 113  12  28   7  42]\n",
      " [  1  33   8  44   4  21]\n",
      " [  0  29  11  61   6  36]\n",
      " [  3  22   8  53   2  12]\n",
      " [  3   4   6  27   2   5]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/5\n",
      "Train loss: 1.632261, Train accuracy: 0.2941\n",
      "Val loss: 2.218692, Val accuracy: 0.4545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|████████████████████████| 4/4 [00:08<00:00,  2.03s/it, accuracy=422, loss=1.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 48  25   0   1   0   0]\n",
      " [  1 191   6   6   1   0]\n",
      " [  0  30  25  49   7   0]\n",
      " [  0   5  11 106  21   0]\n",
      " [  0   0   2  49  49   0]\n",
      " [  0   9   0  15  20   3]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/5\n",
      "Train loss: 1.130980, Train accuracy: 0.6206\n",
      "Val loss: 3.177574, Val accuracy: 0.4463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|███████████████████████| 4/4 [00:07<00:00,  1.98s/it, accuracy=537, loss=0.761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 69   4   0   1   0   0]\n",
      " [  4 191  10   0   0   0]\n",
      " [  1  20  71  19   0   0]\n",
      " [  0   1  13 122   7   0]\n",
      " [  0   0   2  32  66   0]\n",
      " [  0   1   0   9  19  18]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/5\n",
      "Train loss: 0.807838, Train accuracy: 0.7897\n",
      "Val loss: 6.155779, Val accuracy: 0.4215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|███████████████████████| 4/4 [00:08<00:00,  2.14s/it, accuracy=629, loss=0.625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 70   3   1   0   0   0]\n",
      " [  3 196   6   0   0   0]\n",
      " [  0   3 107   1   0   0]\n",
      " [  0   1   8 127   7   0]\n",
      " [  0   0   0   7  88   5]\n",
      " [  0   0   0   0   6  41]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/5\n",
      "Train loss: 0.611479, Train accuracy: 0.9250\n",
      "Val loss: 6.687806, Val accuracy: 0.4380\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train progress: 100%|███████████████████████| 4/4 [00:08<00:00,  2.01s/it, accuracy=651, loss=0.498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion matrix :\n",
      "[[ 74   0   0   0   0   0]\n",
      " [  3 199   3   0   0   0]\n",
      " [  0   4 103   4   0   0]\n",
      " [  0   0   4 133   6   0]\n",
      " [  0   0   0   4  95   1]\n",
      " [  0   0   0   0   0  47]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 5/5\n",
      "Train loss: 0.477685, Train accuracy: 0.9574\n",
      "Val loss: 7.743719, Val accuracy: 0.4380\n",
      "\n",
      "[INFO] Network evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test progress: 100%|███████████████████████████████████████| 2/2 [00:01<00:00,  1.31it/s, loss=2.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix :\n",
      "[[11 11  0  0  0  0]\n",
      " [ 3 40  3 14  0  0]\n",
      " [ 0 12  4 17  0  0]\n",
      " [ 0  5  3 34  0  0]\n",
      " [ 0  2  0 28  0  0]\n",
      " [ 0  0  1 12  0  1]]\n",
      "\n",
      "MS: 0.0000\n",
      "\n",
      "QWK: 0.6607\n",
      "\n",
      "MAE: 0.7363\n",
      "\n",
      "CCR: 0.4478\n",
      "\n",
      "1-off: 0.8308\n",
      "\n",
      "[INFO] Total training time: 48.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "H = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "# To store validation metrics\n",
    "validation_metrics = {}\n",
    "\n",
    "# Definition to store best model weights\n",
    "best_model_weights = model.state_dict()\n",
    "best_qwk = 0.0\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(optimiser_params[\"epochs\"]):\n",
    "    train_acc, train_loss = train(\n",
    "        train_dataloader, model, loss_fn, optimizer, device, H, num_classes=num_classes\n",
    "    )\n",
    "    validation_metrics, val_acc, val_loss = validate(\n",
    "        val_dataloader, model, loss_fn, device, H, num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    if validation_metrics[\"QWK\"] >= best_qwk:\n",
    "        best_qwk = validation_metrics[\"QWK\"]\n",
    "        best_model_weights = deepcopy(model.state_dict())\n",
    "\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, optimiser_params[\"epochs\"]))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(train_loss, train_acc))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(val_loss, val_acc))\n",
    "\n",
    "# Store last train error\n",
    "train_error = H[\"train_loss\"][-1]\n",
    "\n",
    "# Restore best weights\n",
    "model.load_state_dict(best_model_weights)\n",
    "\n",
    "# Start evaluation\n",
    "print(\"[INFO] Network evaluation ...\")\n",
    "\n",
    "test_metrics, test_loss = test(\n",
    "    test_dataloader, model, loss_fn, device, num_classes=num_classes\n",
    ")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(\"\\n[INFO] Total training time: {:.2f}s\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlordinal-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
