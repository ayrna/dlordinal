{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eced7c41a95c3c",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "80b8365a0a058c6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T07:15:07.469099Z",
     "start_time": "2025-11-05T07:15:00.795363Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from skorch.dataset import ValidSplit\n",
    "from torch import cuda, nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "from dlordinal.datasets import FGNet\n",
    "from dlordinal.losses import (\n",
    "    BetaLoss,\n",
    "    BinomialLoss,\n",
    "    EMDLoss,\n",
    "    GeometricLoss,\n",
    "    TriangularLoss,\n",
    "    WKLoss,\n",
    ")\n",
    "from dlordinal.metrics import accuracy_off1, amae, mmae, ranked_probability_score\n",
    "from dlordinal.output_layers import COPOC"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c1dedbfa8ce0cec0",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Download `FGNet` dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "61f107393309ab41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T07:15:07.674235Z",
     "start_time": "2025-11-05T07:15:07.659703Z"
    }
   },
   "source": [
    "fgnet_train = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "fgnet_test = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=Compose([ToTensor()]),\n",
    ")\n",
    "\n",
    "num_classes = len(fgnet_train.classes)\n",
    "classes = fgnet_train.classes\n",
    "targets = fgnet_train.targets\n",
    "\n",
    "# Get CUDA device\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Using cpu device\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "33b26a09f390e9f9",
   "metadata": {},
   "source": [
    "## Metrics \n",
    "Metrics to evaluate different ordinal losses."
   ]
  },
  {
   "cell_type": "code",
   "id": "ecf46fcab23e9fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T07:15:07.736808Z",
     "start_time": "2025-11-05T07:15:07.713817Z"
    }
   },
   "source": [
    "def is_unimodal(probs):\n",
    "    \"\"\"Check if a 1D array is unimodal (increases to a peak, then decreases).\"\"\"\n",
    "    peak_idx = np.argmax(probs)\n",
    "    # Increasing up to peak\n",
    "    inc = np.all(np.diff(probs[: peak_idx + 1]) >= 0)\n",
    "    # Decreasing after peak\n",
    "    dec = np.all(np.diff(probs[peak_idx:]) <= 0)\n",
    "    return inc and dec\n",
    "\n",
    "\n",
    "def check_unimodality(y_pred):\n",
    "    \"\"\"Check unimodality for each row in y_pred and return the proportion.\"\"\"\n",
    "    unimodal_flags = np.array([is_unimodal(row) for row in y_pred])\n",
    "    # Proportion of rows that are unimodal\n",
    "    proportion = np.mean(unimodal_flags)\n",
    "    print(\n",
    "        f\"Unimodal predictions: {np.sum(unimodal_flags)} / {len(y_pred)} ({proportion})\"\n",
    "    )\n",
    "    return proportion\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "\n",
    "    if np.allclose(np.sum(y_pred, axis=1), 1):\n",
    "        y_pred_proba = y_pred\n",
    "    else:\n",
    "        y_pred_proba = softmax(y_pred, axis=1)\n",
    "\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Metrics\n",
    "    amae_metric = amae(y_true, y_pred)\n",
    "    mmae_metric = mmae(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred_max)\n",
    "    acc = accuracy_score(y_true, y_pred_max)\n",
    "    acc_1off = accuracy_off1(y_true, y_pred)\n",
    "    qwk = cohen_kappa_score(y_true, y_pred_max, weights=\"quadratic\")\n",
    "    rps = ranked_probability_score(y_true, y_pred_proba)\n",
    "    # Check unimodality\n",
    "    unimodal_prop = check_unimodality(y_pred_proba)\n",
    "\n",
    "    metrics = {\n",
    "        \"ACC\": acc,\n",
    "        \"1OFF\": acc_1off,\n",
    "        \"MAE\": mae,\n",
    "        \"QWK\": qwk,\n",
    "        \"AMAE\": amae_metric,\n",
    "        \"MMAE\": mmae_metric,\n",
    "        \"RPS\": rps,\n",
    "        \"Unimodality\": unimodal_prop,\n",
    "    }\n",
    "\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred_max))\n",
    "\n",
    "    return metrics"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "505180c5d9115046",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "We want to do a brief comparison of several (ordinal) losses using `PyTorch` and `Skorch`\n",
    "with ResNet18, a pre-trained convolutional neural network, as the model architecture. \n",
    "Concretely, we compare Cross Entropy (CE) Loss with several ordinal approaches from the dlordinal library:\n",
    "\n",
    "- Cross Entropy (CE) Loss\n",
    "- Squared Earth Mover's Distance (EMD) Loss [1]\n",
    "- Weighted Kappa Loss [2]\n",
    "- Binomial Cross Entropy Loss  [3]\n",
    "- Triangular Cross Entropy Loss [4]\n",
    "- Beta Cross Entropy Loss [5]\n",
    "- Geometric Cross Entropy Loss [6]\n",
    "- Conformal prediction sets for ordinal classification (COPOC) [7]\n",
    "\n",
    "[1] Hou, L., Yu, C. P., & Samaras, D. (2016). Squared earth mover's distance-based loss for training deep neural networks. arXiv preprint arXiv:1611.05916. \n",
    "\n",
    "[2] de La Torre, J., Puig, D., & Valls, A. (2018). Weighted kappa loss function for multi-class classification of ordinal data in deep learning. Pattern Recognition Letters, 105, 144-154.\n",
    "\n",
    "[3] Liu, X., Fan, F., Kong, L., Diao, Z., Xie, W., Lu, J., & You, J. (2020). Unimodal regularized neuron stick-breaking for ordinal classification. Neurocomputing, 388, 34-44.\n",
    "\n",
    "[4] Vargas, V. M., Gutiérrez, P. A., Barbero-Gómez, J., & Hervás-Martínez, C. (2023). Soft labelling based on triangular distributions for ordinal classification. Information Fusion, 93, 258-267.\n",
    "\n",
    "[5] Vargas, V. M., Gutiérrez, P. A., & Hervás-Martínez, C. (2022). Unimodal regularisation based on beta distribution for deep ordinal regression. Pattern Recognition, 122, 108310.\n",
    "\n",
    "[6] Haas, S., & Hüllermeier, E. (2023, September). Rectifying bias in ordinal observational data using unimodal label smoothing. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 3-18). Cham: Springer Nature Switzerland.\n",
    "\n",
    "[7] Dey, P., Merugu, S., & Kaveri, S. R. (2023). Conformal prediction sets for ordinal classification. Advances in Neural Information Processing Systems, 36, 879-899.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4513ef5e1f1abca",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-05T10:55:30.824103Z"
    }
   },
   "source": [
    "# Loss functions\n",
    "losses = [\n",
    "    CrossEntropyLoss().to(device),\n",
    "    COPOC().to(device),\n",
    "    EMDLoss(num_classes=num_classes).to(device),\n",
    "    WKLoss(num_classes=num_classes, use_logits=True).to(device),\n",
    "    BinomialLoss(base_loss=nn.CrossEntropyLoss(), num_classes=num_classes).to(device),\n",
    "    TriangularLoss(base_loss=nn.CrossEntropyLoss(), num_classes=num_classes).to(device),\n",
    "    BetaLoss(base_loss=nn.CrossEntropyLoss(), num_classes=num_classes).to(device),\n",
    "    GeometricLoss(\n",
    "        base_loss=nn.CrossEntropyLoss(),\n",
    "        num_classes=num_classes,\n",
    "        alphas=[0.15, 0.35, 0.35, 0.35, 0.35, 0.15],\n",
    "    ).to(device),\n",
    "]\n",
    "\n",
    "# Evaluate each loss K times with different seeds to obtain a more robust result\n",
    "K = 3\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Add deterministic settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "for loss_fn in losses:\n",
    "\n",
    "    for k in range(K):\n",
    "        # Make results reproducible\n",
    "        seed = k\n",
    "        set_seed(seed)  # Use K different seeds\n",
    "\n",
    "        # Initialize ResNet18 model\n",
    "        model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        if type(loss_fn).__name__ == \"COPOC\":\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Linear(model.fc.in_features, num_classes), loss_fn\n",
    "            )\n",
    "            loss = CrossEntropyLoss().to(device)\n",
    "        else:\n",
    "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "            loss = loss_fn\n",
    "        model.to(device)\n",
    "\n",
    "        # Skorch estimator\n",
    "        estimator = NeuralNetClassifier(\n",
    "            module=model,\n",
    "            criterion=loss,\n",
    "            optimizer=Adam,\n",
    "            lr=0.001,\n",
    "            max_epochs=30,\n",
    "            # verbose=0,\n",
    "            train_split=ValidSplit(\n",
    "                0.1, random_state=seed\n",
    "            ),  # Use 10% of the data for validation\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=5, monitor=\"valid_loss\"),\n",
    "                LRScheduler(policy=ReduceLROnPlateau, patience=3, factor=0.5),\n",
    "            ],\n",
    "            device=device,\n",
    "            batch_size=200,\n",
    "        )\n",
    "\n",
    "        print(\"#\" + str(k) + \" \" + type(loss_fn).__name__)\n",
    "\n",
    "        estimator.fit(\n",
    "            X=fgnet_train, y=torch.tensor(fgnet_train.targets, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "        test_probs = estimator.predict_proba(fgnet_test)\n",
    "\n",
    "        metrics = calculate_metrics(np.array(fgnet_test.targets), test_probs)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        df = pd.DataFrame([metrics])\n",
    "        df[\"iteration\"] = k\n",
    "        df[\"loss\"] = type(loss_fn).__name__\n",
    "\n",
    "        result = pd.concat([result, df], ignore_index=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 CrossEntropyLoss\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.6402\u001b[0m       \u001b[32m0.2593\u001b[0m        \u001b[35m1.9679\u001b[0m  13.6865\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df723be940592ba3",
   "metadata": {},
   "source": [
    "## Result\n",
    "Displays the mean results for each loss over the K iterations."
   ]
  },
  {
   "cell_type": "code",
   "id": "30fc9a09232b78e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T10:27:58.372863Z",
     "start_time": "2025-11-05T10:27:57.977326Z"
    }
   },
   "source": [
    "result.set_index([\"loss\", \"iteration\"], inplace=True)\n",
    "result.groupby(\"loss\").agg(\"mean\").style.highlight_max(\n",
    "    axis=0, subset=[\"ACC\", \"1OFF\", \"QWK\"], color=\"green\"\n",
    ").highlight_min(axis=0, subset=[\"MAE\", \"AMAE\", \"MMAE\", \"RPS\"], color=\"green\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x135a4ef2230>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_38ac8_row0_col1, #T_38ac8_row1_col1, #T_38ac8_row2_col5, #T_38ac8_row6_col0, #T_38ac8_row6_col4, #T_38ac8_row6_col6, #T_38ac8_row7_col2, #T_38ac8_row7_col3 {\n",
       "  background-color: green;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_38ac8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_38ac8_level0_col0\" class=\"col_heading level0 col0\" >ACC</th>\n",
       "      <th id=\"T_38ac8_level0_col1\" class=\"col_heading level0 col1\" >1OFF</th>\n",
       "      <th id=\"T_38ac8_level0_col2\" class=\"col_heading level0 col2\" >MAE</th>\n",
       "      <th id=\"T_38ac8_level0_col3\" class=\"col_heading level0 col3\" >QWK</th>\n",
       "      <th id=\"T_38ac8_level0_col4\" class=\"col_heading level0 col4\" >AMAE</th>\n",
       "      <th id=\"T_38ac8_level0_col5\" class=\"col_heading level0 col5\" >MMAE</th>\n",
       "      <th id=\"T_38ac8_level0_col6\" class=\"col_heading level0 col6\" >RPS</th>\n",
       "      <th id=\"T_38ac8_level0_col7\" class=\"col_heading level0 col7\" >Unimodality</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >loss</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row0\" class=\"row_heading level0 row0\" >BetaLoss</th>\n",
       "      <td id=\"T_38ac8_row0_col0\" class=\"data row0 col0\" >0.570481</td>\n",
       "      <td id=\"T_38ac8_row0_col1\" class=\"data row0 col1\" >0.950249</td>\n",
       "      <td id=\"T_38ac8_row0_col2\" class=\"data row0 col2\" >0.482587</td>\n",
       "      <td id=\"T_38ac8_row0_col3\" class=\"data row0 col3\" >0.839975</td>\n",
       "      <td id=\"T_38ac8_row0_col4\" class=\"data row0 col4\" >0.552453</td>\n",
       "      <td id=\"T_38ac8_row0_col5\" class=\"data row0 col5\" >0.904762</td>\n",
       "      <td id=\"T_38ac8_row0_col6\" class=\"data row0 col6\" >0.341949</td>\n",
       "      <td id=\"T_38ac8_row0_col7\" class=\"data row0 col7\" >0.912106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row1\" class=\"row_heading level0 row1\" >BinomialLoss</th>\n",
       "      <td id=\"T_38ac8_row1_col0\" class=\"data row1 col0\" >0.553897</td>\n",
       "      <td id=\"T_38ac8_row1_col1\" class=\"data row1 col1\" >0.950249</td>\n",
       "      <td id=\"T_38ac8_row1_col2\" class=\"data row1 col2\" >0.500829</td>\n",
       "      <td id=\"T_38ac8_row1_col3\" class=\"data row1 col3\" >0.832931</td>\n",
       "      <td id=\"T_38ac8_row1_col4\" class=\"data row1 col4\" >0.545851</td>\n",
       "      <td id=\"T_38ac8_row1_col5\" class=\"data row1 col5\" >0.880952</td>\n",
       "      <td id=\"T_38ac8_row1_col6\" class=\"data row1 col6\" >0.398601</td>\n",
       "      <td id=\"T_38ac8_row1_col7\" class=\"data row1 col7\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row2\" class=\"row_heading level0 row2\" >COPOC</th>\n",
       "      <td id=\"T_38ac8_row2_col0\" class=\"data row2 col0\" >0.548922</td>\n",
       "      <td id=\"T_38ac8_row2_col1\" class=\"data row2 col1\" >0.912106</td>\n",
       "      <td id=\"T_38ac8_row2_col2\" class=\"data row2 col2\" >0.550580</td>\n",
       "      <td id=\"T_38ac8_row2_col3\" class=\"data row2 col3\" >0.805266</td>\n",
       "      <td id=\"T_38ac8_row2_col4\" class=\"data row2 col4\" >0.558297</td>\n",
       "      <td id=\"T_38ac8_row2_col5\" class=\"data row2 col5\" >0.788240</td>\n",
       "      <td id=\"T_38ac8_row2_col6\" class=\"data row2 col6\" >0.440549</td>\n",
       "      <td id=\"T_38ac8_row2_col7\" class=\"data row2 col7\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row3\" class=\"row_heading level0 row3\" >CrossEntropyLoss</th>\n",
       "      <td id=\"T_38ac8_row3_col0\" class=\"data row3 col0\" >0.517413</td>\n",
       "      <td id=\"T_38ac8_row3_col1\" class=\"data row3 col1\" >0.887231</td>\n",
       "      <td id=\"T_38ac8_row3_col2\" class=\"data row3 col2\" >0.618574</td>\n",
       "      <td id=\"T_38ac8_row3_col3\" class=\"data row3 col3\" >0.751100</td>\n",
       "      <td id=\"T_38ac8_row3_col4\" class=\"data row3 col4\" >0.692641</td>\n",
       "      <td id=\"T_38ac8_row3_col5\" class=\"data row3 col5\" >1.238817</td>\n",
       "      <td id=\"T_38ac8_row3_col6\" class=\"data row3 col6\" >0.518172</td>\n",
       "      <td id=\"T_38ac8_row3_col7\" class=\"data row3 col7\" >0.341625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row4\" class=\"row_heading level0 row4\" >EMDLoss</th>\n",
       "      <td id=\"T_38ac8_row4_col0\" class=\"data row4 col0\" >0.562189</td>\n",
       "      <td id=\"T_38ac8_row4_col1\" class=\"data row4 col1\" >0.912106</td>\n",
       "      <td id=\"T_38ac8_row4_col2\" class=\"data row4 col2\" >0.542289</td>\n",
       "      <td id=\"T_38ac8_row4_col3\" class=\"data row4 col3\" >0.793638</td>\n",
       "      <td id=\"T_38ac8_row4_col4\" class=\"data row4 col4\" >0.613336</td>\n",
       "      <td id=\"T_38ac8_row4_col5\" class=\"data row4 col5\" >1.120635</td>\n",
       "      <td id=\"T_38ac8_row4_col6\" class=\"data row4 col6\" >0.441204</td>\n",
       "      <td id=\"T_38ac8_row4_col7\" class=\"data row4 col7\" >0.497512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row5\" class=\"row_heading level0 row5\" >GeometricLoss</th>\n",
       "      <td id=\"T_38ac8_row5_col0\" class=\"data row5 col0\" >0.583748</td>\n",
       "      <td id=\"T_38ac8_row5_col1\" class=\"data row5 col1\" >0.948590</td>\n",
       "      <td id=\"T_38ac8_row5_col2\" class=\"data row5 col2\" >0.477612</td>\n",
       "      <td id=\"T_38ac8_row5_col3\" class=\"data row5 col3\" >0.838234</td>\n",
       "      <td id=\"T_38ac8_row5_col4\" class=\"data row5 col4\" >0.530820</td>\n",
       "      <td id=\"T_38ac8_row5_col5\" class=\"data row5 col5\" >0.810245</td>\n",
       "      <td id=\"T_38ac8_row5_col6\" class=\"data row5 col6\" >0.358396</td>\n",
       "      <td id=\"T_38ac8_row5_col7\" class=\"data row5 col7\" >0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row6\" class=\"row_heading level0 row6\" >TriangularLoss</th>\n",
       "      <td id=\"T_38ac8_row6_col0\" class=\"data row6 col0\" >0.605307</td>\n",
       "      <td id=\"T_38ac8_row6_col1\" class=\"data row6 col1\" >0.936982</td>\n",
       "      <td id=\"T_38ac8_row6_col2\" class=\"data row6 col2\" >0.467662</td>\n",
       "      <td id=\"T_38ac8_row6_col3\" class=\"data row6 col3\" >0.837905</td>\n",
       "      <td id=\"T_38ac8_row6_col4\" class=\"data row6 col4\" >0.507985</td>\n",
       "      <td id=\"T_38ac8_row6_col5\" class=\"data row6 col5\" >0.789177</td>\n",
       "      <td id=\"T_38ac8_row6_col6\" class=\"data row6 col6\" >0.341560</td>\n",
       "      <td id=\"T_38ac8_row6_col7\" class=\"data row6 col7\" >0.774461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_38ac8_level0_row7\" class=\"row_heading level0 row7\" >WKLoss</th>\n",
       "      <td id=\"T_38ac8_row7_col0\" class=\"data row7 col0\" >0.598673</td>\n",
       "      <td id=\"T_38ac8_row7_col1\" class=\"data row7 col1\" >0.941957</td>\n",
       "      <td id=\"T_38ac8_row7_col2\" class=\"data row7 col2\" >0.462687</td>\n",
       "      <td id=\"T_38ac8_row7_col3\" class=\"data row7 col3\" >0.843138</td>\n",
       "      <td id=\"T_38ac8_row7_col4\" class=\"data row7 col4\" >0.526022</td>\n",
       "      <td id=\"T_38ac8_row7_col5\" class=\"data row7 col5\" >1.026984</td>\n",
       "      <td id=\"T_38ac8_row7_col6\" class=\"data row7 col6\" >0.418116</td>\n",
       "      <td id=\"T_38ac8_row7_col7\" class=\"data row7 col7\" >0.606965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlordinal-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
