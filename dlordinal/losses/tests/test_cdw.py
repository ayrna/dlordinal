import numpy as np
import pytest
import torch
from torch import nn

from dlordinal.losses import CDWCELoss


@pytest.fixture
def device():
    return "cuda" if torch.cuda.is_available() else "cpu"


def cdwce_loop_softmax(y_pred, y_true, alpha, device):
    y_pred = nn.functional.softmax(y_pred, dim=1).to(device)
    y_true = y_true.to(device)
    N = y_true.size(0)
    J = y_true.size(1)

    loss = 0
    for i in range(N):
        for j in range(J):
            loss += (
                torch.log(1.0 - y_pred[i, j] + 1e-8).to(device)
                * torch.abs(j - torch.argmax(y_true[i])).to(device) ** alpha
            )
    return -loss / N


def cdwce_vect_softmax(y_pred, y_true, alpha, device):
    y_pred = nn.functional.softmax(y_pred, dim=1).to(device)
    y_true = y_true.to(device)

    N = y_true.size(0)
    J = y_true.size(1)

    # Create a matrix of indices
    indices = torch.arange(J).unsqueeze(0).repeat(N, 1).to(device)
    # Get the true class indices
    true_indices = torch.argmax(y_true, dim=1).unsqueeze(1).to(device)
    # Calculate the absolute differences
    abs_diff = torch.abs(indices - true_indices).to(device)
    # Calculate the loss
    loss = torch.log(1.0 - y_pred + 1e-8).to(device) * (abs_diff**alpha)
    return -loss.sum() / N


def cdwce_loop(y_pred, y_true, alpha, device):
    N = y_true.size(0)
    J = y_true.size(1)

    y_true = y_true.to(device)
    y_pred = y_pred.to(device)

    loss = 0.0
    for n in range(N):
        for i in range(J):
            s = 0
            for j in range(J):
                s += torch.exp(y_pred[n, j]).to(device)

            l1 = torch.log(s - torch.exp(y_pred[n, i])).to(device)
            l2 = torch.log(s).to(device)
            l_1_2 = l1 - l2
            loss += l_1_2 * (abs(i - torch.argmax(y_true[n])).to(device) ** alpha)

    return -loss / N


def test_cdwce_exact_value(device):
    for alpha, result in [(0.5, 1.73862), (1.0, 2.37753), (2.0, 4.99029)]:
        loss = CDWCELoss(num_classes=4, alpha=alpha).to(device)
        y_true = torch.tensor(
            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]
        ).to(device)
        y_pred = torch.tensor(
            [[1, 2, 3, 4], [5, 8, 1, 0], [-2, 5, 3, 2], [-4, 5, 3, 2]]
        ).to(device)
        loss_value = loss(y_pred, y_true)
        assert loss_value.item() == pytest.approx(result, abs=1e-4)


def test_cdwce_exact_value_class_weights(device):
    tests = [
        (0.5, torch.tensor([5, 1, 1.1, 6.6]), 0.57611),
        (1.0, torch.tensor([5, 1, 1.1, 6.6]), 0.84099),
        (2.0, torch.tensor([5, 1, 1.1, 6.6]), 1.90280),
        (0.5, torch.tensor([1, 2, 3, 4]), 0.47590),
        (1.0, torch.tensor([1, 2, 3, 4]), 0.62137),
        (2.0, torch.tensor([1, 2, 3, 4]), 1.16119),
    ]
    for alpha, weight, result in tests:
        loss = CDWCELoss(num_classes=4, alpha=alpha, weight=weight).to(device)
        y_true = torch.tensor(
            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]
        ).to(device)
        y_pred = torch.tensor(
            [[1, 2, 3, 4], [5, 8, 1, 0], [-2, 5, 3, 2], [-4, 5, 3, 2]]
        ).to(device)
        loss_value = loss(y_pred, y_true)
        assert loss_value.item() == pytest.approx(result, abs=1e-4)


def test_cdwce_compare_different_versions(device):
    alpha = 0.5
    loss = CDWCELoss(num_classes=4, alpha=alpha).to(device)
    y_true = torch.tensor([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]).to(
        device
    )
    y_pred = torch.tensor(
        [[1, 0, 0, 0], [0.2, 0.3, 0.5, 0], [0.7, 0.1, 0.1, 0.1], [0, 0, 0.05, 0.95]]
    ).to(device)
    loss_value = loss(y_pred, y_true)

    loss_value_loop_softmax = cdwce_loop_softmax(y_pred, y_true, alpha, device)
    loss_value_vect = cdwce_vect_softmax(y_pred, y_true, alpha, device)
    loss_value_loop = cdwce_loop(y_pred, y_true, alpha, device)

    # First test that all the versions implemented in this test are the same
    assert loss_value_loop_softmax.item() == pytest.approx(
        loss_value_vect.item(), abs=1e-4
    )
    assert loss_value_loop_softmax.item() == pytest.approx(
        loss_value_loop.item(), abs=1e-4
    )
    assert loss_value_vect.item() == pytest.approx(loss_value_loop.item(), abs=1e-4)

    # Then compare these versions with the one implemented in the library
    assert loss_value.item() == pytest.approx(loss_value_loop_softmax.item(), abs=1e-4)
    assert loss_value.item() == pytest.approx(loss_value_vect.item(), abs=1e-4)
    assert loss_value.item() == pytest.approx(loss_value_loop.item(), abs=1e-4)


def test_cdwce_random_y(device):
    for alpha in np.linspace(0.05, 3.0, 10):
        loss = CDWCELoss(num_classes=10, alpha=alpha).to(device)
        y_true = torch.tensor(
            [
                [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0],
                [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],
                [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0],
                [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0],
                [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0],
                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0],
                [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0],
                [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0],
                [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],
                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0],
                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0],
                [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0],
                [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0],
                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0],
                [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0],
                [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0],
                [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0],
                [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
                [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],
                [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],
                [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0],
                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],
                [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
                [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0],
                [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0],
                [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0],
                [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0],
                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0],
                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0],
                [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0],
                [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0],
                [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0],
                [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0],
                [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0],
                [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0],
                [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0],
                [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],
                [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0],
                [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],
                [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0],
                [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
                [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],
                [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0],
                [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],
                [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0],
                [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0],
                [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0],
                [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0],
                [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0],
                [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0],
                [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0],
                [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0],
                [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0],
                [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0],
                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],
                [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
                [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0],
                [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0],
                [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0],
                [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0],
                [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0],
                [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
                [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0],
                [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],
                [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
                [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0],
                [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],
            ]
        ).to(device)
        # fmt: off
        y_pred = torch.tensor(
            [[-7.4033e-01, -7.5837e-02, -1.9534e+00, -4.3210e-01, -1.2681e+00,
            3.5143e-01, -8.0105e-01,  1.0087e-01,  6.9479e-01, -7.8526e-01],
            [-2.0024e+00, -8.6025e-01, -2.6987e-01,  2.2336e-01,  1.4908e+00,
            1.0347e-01, -2.3418e+00, -1.3716e+00, -8.8763e-01, -5.6791e-01],
            [ 1.1281e-01,  6.3613e-01, -5.5082e-01,  1.5715e+00, -1.0298e+00,
            -3.5456e-01,  2.6935e+00,  5.0846e-01, -2.5938e-01,  7.4629e-01],
            [ 1.3060e+00,  6.4125e-02, -5.4776e-02, -2.5541e-01,  4.0879e-01,
            5.3934e-01, -1.5511e+00,  4.9498e-01, -4.6819e-01,  1.7644e-02],
            [ 7.6073e-01, -8.7380e-01,  1.4699e+00,  1.8162e+00,  1.3599e+00,
            1.1513e+00,  1.7086e+00, -7.9322e-01, -9.0940e-01, -5.3335e-01],
            [-7.4273e-01,  4.2641e-01, -7.3766e-01,  1.9391e-01,  2.4286e-01,
            5.8392e-01, -1.7986e-01, -1.3283e+00, -3.1872e-01, -9.2277e-01],
            [-9.2988e-01,  3.8044e-01,  1.8463e+00,  2.8676e-01,  4.8036e-01,
            3.0825e+00,  2.7852e-01, -2.4583e-01, -9.1003e-01, -3.6813e-01],
            [-3.8634e-01,  5.3613e-01, -2.6867e-03,  1.2980e+00,  3.4414e+00,
            1.9090e-01, -3.6723e-01,  5.3231e-01, -1.4559e+00, -8.8626e-01],
            [ 1.7149e+00,  2.7099e-02,  2.8199e-01, -4.6614e-01, -2.9038e-01,
            -4.2140e-01, -1.6952e-01, -4.9593e-01, -4.2830e-02, -5.7157e-01],
            [ 5.1639e-01,  6.0285e-01, -1.2013e+00, -4.8872e-01,  4.9785e-01,
            2.3845e-01,  1.3846e+00,  1.1936e+00, -2.9293e-01, -2.9374e-01],
            [ 1.3968e+00, -7.5835e-01,  1.0405e+00, -2.4384e-01,  2.3231e-01,
            9.4262e-01,  1.1669e+00,  6.8784e-01,  1.0670e+00,  5.9488e-02],
            [ 7.8581e-02,  4.8099e-01,  6.4138e-01,  1.9595e-02, -6.8808e-01,
            1.4677e-01, -1.1260e+00, -2.9445e-01,  5.4796e-02,  4.0955e-01],
            [-1.4105e-01,  1.6325e+00,  1.1941e+00,  4.5925e-01, -1.2297e-01,
            1.0144e+00,  9.9960e-02, -6.2733e-01,  2.5031e-02, -6.7940e-02],
            [-8.0089e-01, -8.3666e-01,  1.0472e+00, -1.4965e+00,  6.1211e-01,
            -1.2478e+00, -3.3329e+00, -7.3981e-01, -7.5972e-01,  7.3397e-01],
            [ 5.9525e-01,  1.7740e+00, -1.8594e+00, -1.0748e+00,  1.1253e+00,
            -5.3914e-01, -4.2645e-02, -3.1493e-01,  2.0031e-01, -6.9216e-01],
            [ 9.7263e-02,  1.2906e+00,  5.2831e-01,  4.7412e-01,  1.0785e+00,
            2.5674e-01,  1.1223e+00, -3.3948e-01,  1.2852e+00,  1.7031e+00],
            [ 6.4474e-01, -1.1425e+00,  6.3175e-01, -1.3863e-01, -4.1514e-01,
            -1.2799e+00, -9.0031e-01, -1.2107e+00,  1.6923e+00,  9.0910e-01],
            [-5.8388e-01, -1.0354e+00, -7.6834e-01, -1.9111e-01, -4.8850e-01,
            2.1789e-01,  9.9590e-01,  3.9482e-01, -4.2135e-01, -7.1878e-01],
            [-5.2997e-01, -1.7315e-02,  4.6528e-02, -5.3971e-01,  1.7235e+00,
            -4.0704e-01,  1.0073e+00, -1.3196e+00, -1.2468e-01, -7.6105e-01],
            [-1.2280e-01,  1.4119e+00, -3.9354e-01,  1.0174e+00,  2.4567e-01,
            3.6104e-01,  4.5374e-01, -4.7674e-01, -1.3438e+00,  6.6690e-01],
            [-8.8973e-01, -5.3083e-01,  3.9391e-01, -6.8078e-01, -9.3998e-01,
            -7.3523e-01,  4.0015e-01,  2.7967e-01, -7.6282e-01,  4.7402e-01],
            [ 1.9302e+00, -3.1939e-01,  1.6842e-01,  2.4912e-01, -2.7114e-01,
            5.8671e-01, -8.1173e-01, -1.7227e-01,  2.7240e-01,  1.5007e+00],
            [ 8.4907e-01, -4.7794e-01, -3.9499e-02, -1.9763e+00,  1.1584e+00,
            -1.4684e+00,  2.3116e-01, -5.6047e-01, -5.3184e-01,  1.0508e+00],
            [-9.2850e-01, -1.3293e+00, -1.6261e-01, -6.4113e-01, -3.3054e+00,
            -6.8585e-01,  6.6536e-01,  1.0991e+00,  9.8839e-01,  1.3725e-01],
            [ 4.2965e-01,  4.0572e-01,  7.5647e-01, -1.9756e+00, -1.7070e+00,
            7.8222e-01,  1.7784e-01,  2.3818e-01,  3.8531e-01,  8.3368e-01],
            [-2.4634e-01,  2.9187e-01, -1.1756e+00, -2.5641e+00,  9.8167e-01,
            -1.0119e+00, -6.6730e-01,  3.4576e-01, -6.5933e-01,  5.2013e-02],
            [-4.1948e-02,  1.1547e+00,  8.4191e-01, -3.8843e-01, -7.7256e-01,
            -1.3458e+00,  1.5988e+00, -1.5566e+00,  2.3216e+00,  4.8033e-01],
            [-7.5121e-01, -3.5991e-01,  1.0760e+00, -2.8951e-01,  1.9765e+00,
            5.3459e-02, -5.1131e-01, -9.8954e-01,  1.8591e-01, -1.7558e+00],
            [ 3.9017e-01, -1.4714e+00,  2.1706e+00, -1.7193e+00,  1.2775e-01,
            1.8893e-01, -8.6175e-01, -9.6621e-01,  1.1458e+00, -7.6786e-01],
            [ 4.5294e-01,  1.3442e+00,  1.8727e-01,  8.2990e-01, -1.9108e+00,
            -2.5170e-01, -8.7860e-01, -1.1308e+00,  3.4723e-01,  5.6391e-01],
            [ 6.3205e-03,  7.5435e-01, -1.3444e+00, -6.8699e-02,  8.0241e-01,
            -4.2806e-01,  9.5963e-01, -3.9059e-01,  9.8809e-01, -7.8315e-01],
            [ 4.5079e-01,  1.3598e+00,  1.0147e+00,  8.9806e-01, -1.1405e+00,
            2.1165e+00, -2.1032e-01, -9.4157e-01, -7.0149e-01,  1.7981e-01],
            [-5.9558e-01,  9.8652e-01,  1.7183e+00,  1.7094e+00, -2.2218e+00,
            -4.5071e-01, -1.0345e+00, -1.3073e+00, -1.5080e-01,  2.8953e+00],
            [ 9.6238e-02, -7.5477e-01, -1.2344e-01, -7.3021e-01, -8.1998e-01,
            4.8434e-01, -6.2256e-01, -7.0768e-02,  6.4770e-01, -1.8574e-01],
            [ 1.1489e+00, -2.0148e-02, -3.0405e-01,  1.2279e+00, -8.1808e-01,
            7.7546e-01, -2.5835e+00,  2.2721e-01,  2.7611e-01,  1.8818e+00],
            [-8.0453e-01, -1.3135e+00,  9.3374e-02,  8.3712e-01,  4.5960e-01,
            2.3195e-01,  7.0757e-01, -5.9488e-01,  6.2760e-01, -1.9829e+00],
            [-8.5303e-01, -9.2422e-01,  4.9301e-01,  1.4058e-01,  7.9280e-01,
            -2.5337e-01,  1.2961e+00,  4.7525e-01, -9.9407e-01, -1.6072e+00],
            [-7.5555e-01,  3.5030e-01,  7.7176e-01, -1.0956e+00,  1.4459e+00,
            6.8618e-01,  2.9953e-01,  1.5510e+00,  5.0856e-01, -3.8794e-01],
            [ 1.9380e-01, -3.2145e-01,  1.9558e-02, -1.2493e+00, -3.0838e-01,
            -6.9978e-01, -7.8930e-01, -2.5570e+00, -1.2202e+00, -1.0821e+00],
            [ 4.7567e-01,  2.0410e+00, -2.0044e+00,  1.0053e+00, -4.0282e-01,
            3.6934e-01,  1.8837e+00, -2.0280e-01, -2.4887e-01, -8.7656e-01],
            [ 6.6443e-01,  1.2105e+00, -3.0620e-01, -7.8168e-01,  9.1399e-01,
            -6.7937e-01, -1.0064e+00,  4.9633e-01,  4.5695e-01,  5.5848e-01],
            [-4.4650e-01,  3.1462e-01, -2.4843e+00, -7.1155e-01,  1.2098e+00,
            6.3186e-01, -7.8653e-01, -2.0912e+00, -7.6345e-01,  3.5755e-01],
            [ 2.8192e+00, -3.2044e-01, -5.1813e-01,  1.2543e+00,  8.2858e-01,
            -5.4963e-01,  8.6340e-02,  3.0863e-01, -1.1520e+00, -3.3330e-01],
            [-4.4924e-02,  1.6931e+00,  1.7917e+00, -1.2519e+00, -3.9358e-02,
            -8.4474e-01,  1.0745e+00,  9.7454e-01, -9.2959e-01,  1.6647e-01],
            [ 7.3295e-01,  6.1377e-01,  1.3167e+00, -8.2793e-02,  8.0702e-01,
            7.0114e-01,  4.8198e-01,  3.6895e-01,  4.3936e-01,  7.9107e-01],
            [ 2.8798e-01, -9.6551e-03,  9.3638e-01, -1.1409e-01, -4.4115e-01,
            -5.7891e-01, -2.0455e+00,  1.2905e+00, -8.5479e-01, -5.4885e-01],
            [-3.0469e-01, -1.4275e-01,  6.7188e-02,  3.5519e-01, -4.4913e-01,
            5.1030e-02, -1.3988e+00, -1.5412e+00, -4.2816e-01,  1.1239e+00],
            [ 1.1888e+00,  1.4362e+00, -5.7543e-01,  5.9426e-01, -3.2800e-02,
            3.5782e-01, -7.8545e-02,  1.4078e+00, -8.8214e-01,  4.6286e-01],
            [ 9.1918e-01,  1.2119e+00,  5.2896e-01, -5.7043e-01,  1.3834e+00,
            1.3184e+00,  7.7827e-01,  3.8513e-01, -1.9569e-01,  1.4473e+00],
            [ 1.2947e+00, -2.2144e-01,  4.9618e-02, -2.5118e-01,  4.0063e-01,
            7.5100e-01, -2.7010e-01,  2.5770e-01, -2.8352e+00, -8.1888e-01],
            [-8.5644e-01,  1.2900e-01, -2.1421e+00, -9.6933e-01,  1.2248e+00,
            1.6388e+00,  1.2112e+00, -1.1232e+00,  2.7703e-01,  6.5813e-01],
            [-2.7307e-01, -2.3828e+00,  6.9403e-01,  2.2239e+00,  1.2502e+00,
            -3.7547e-01, -1.9565e+00, -6.1771e-01, -2.3523e-01,  1.1666e+00],
            [ 1.4258e+00, -1.4098e+00,  7.3191e-01,  1.1886e+00, -7.0446e-01,
            -2.0173e+00, -9.1518e-01, -6.3333e-01, -1.2781e+00,  1.7970e+00],
            [-3.0139e-01, -5.2288e-01,  2.4269e+00,  1.0921e-01,  1.1948e+00,
            1.5870e+00,  3.0597e-01, -9.9800e-01, -4.0220e-01,  1.1106e+00],
            [ 2.1105e+00, -1.1088e+00, -1.6503e-01, -5.1929e-01,  1.5781e-01,
            -7.6025e-01,  1.5650e+00,  1.4684e+00,  5.6494e-01, -1.0450e+00],
            [-1.1151e+00,  4.2420e-01,  7.4936e-01, -7.2205e-01,  4.8393e-02,
            6.3960e-01,  5.6101e-01, -1.3797e-03, -6.5473e-01, -5.7513e-01],
            [-1.0309e+00,  9.8475e-01, -4.8944e-01,  4.4026e-01,  8.8770e-01,
            1.8215e+00,  1.0649e+00, -1.9322e+00, -1.3428e+00, -4.3385e-02],
            [ 1.8898e+00, -1.1185e-01,  3.5240e-02,  9.3298e-01, -7.0715e-01,
            -7.2429e-01, -5.0578e-01, -9.9425e-01, -2.1477e-01,  3.1920e+00],
            [ 2.1183e-01,  1.2824e+00, -2.3275e-01, -8.3613e-01,  3.5350e-01,
            1.0474e+00,  2.4867e-01, -4.9174e-01,  1.6074e+00,  4.8353e-01],
            [-4.5889e-02, -1.3565e-01, -1.5731e+00, -1.7226e+00,  1.4783e+00,
            -1.5708e+00, -1.4213e+00,  9.9536e-01,  1.1297e+00, -2.9032e+00],
            [-6.3349e-01,  1.1370e-01,  1.0215e+00,  3.5630e-01,  7.9035e-01,
            -2.6674e-01, -1.0821e+00,  1.4136e-01, -5.3575e-01, -9.2955e-01],
            [-1.0399e-01, -7.9695e-01,  1.1925e+00, -3.1238e-01,  3.3576e-01,
            -1.5778e+00, -4.9648e-01,  1.5081e+00, -5.4097e-01,  1.0608e+00],
            [ 1.2742e+00,  1.7510e+00,  2.0986e+00, -2.5632e-01, -1.5592e+00,
            -1.1297e+00, -1.2397e+00, -3.4955e-01,  1.2102e+00, -1.4426e-01],
            [-1.9109e+00, -2.9694e-01, -3.4183e-01, -1.4177e+00,  1.4876e+00,
            7.3172e-01, -9.2566e-01, -1.0349e+00, -2.5474e-01, -1.4676e+00],
            [ 5.9159e-01, -7.3157e-01,  2.4575e-01, -1.1133e+00,  1.4161e+00,
            -1.6493e-01, -4.8860e-01,  1.6133e+00, -1.5106e+00,  7.0435e-01],
            [-1.0606e+00,  2.0211e-01, -6.8995e-01,  2.2150e-01, -3.6488e-01,
            -1.0863e+00, -7.3613e-01, -1.2825e+00, -1.0843e+00, -1.7993e+00],
            [ 2.0739e+00, -1.5149e-01, -7.4225e-01,  2.1909e+00, -5.8212e-01,
            4.7676e-01, -3.0498e-01, -4.0741e-01,  3.4566e-01,  1.1424e-02],
            [ 6.1049e-01,  8.7054e-01,  5.6991e-01, -8.8562e-01,  1.4692e+00,
            4.5958e-01, -1.1191e+00,  1.0039e+00, -4.5426e-01, -6.1964e-01],
            [-7.2138e-01,  2.5514e+00, -1.7300e+00,  2.5063e-01, -1.4354e-01,
            -1.7591e+00,  1.0338e+00, -1.4171e-01, -1.6937e+00,  1.9170e+00],
            [-4.9587e-02,  3.6314e-01,  9.7851e-01,  9.1873e-02, -1.1863e+00,
            -1.2473e+00,  2.3307e+00,  1.8016e+00, -7.8190e-02, -7.4388e-01],
            [ 1.7462e+00, -5.7841e-01, -5.2970e-01,  4.8815e-01,  1.3359e-01,
            -1.5834e+00,  1.7557e+00,  9.6831e-01,  4.0870e-01, -7.7780e-01],
            [-1.0013e+00, -5.1342e-01, -8.2461e-02,  2.9930e+00, -1.4035e+00,
            1.6376e+00,  6.0808e-01,  3.6908e-01,  1.3268e-01,  1.3805e+00],
            [ 1.0553e+00,  6.0455e-01,  1.5442e+00, -6.1557e-01, -7.3410e-01,
            -3.1795e-01, -1.4454e+00, -3.3736e-01,  1.5892e+00, -4.0373e-01],
            [ 5.3686e-01, -1.0334e+00, -2.8692e+00,  1.1368e+00, -4.0583e-01,
            -2.4074e+00, -1.5742e+00,  4.5854e-01,  1.2203e+00,  6.5260e-01],
            [ 3.6168e-01,  2.6635e-01, -4.1593e-01,  1.6803e+00, -1.1224e+00,
            9.0934e-01, -1.5725e+00, -5.1979e-01,  1.2250e+00,  6.6774e-01],
            [ 4.9696e-01,  9.1938e-01, -2.2423e+00,  9.6148e-02, -6.7544e-02,
            -4.9714e-01,  1.6799e+00,  1.9252e+00, -5.6217e-01,  1.4871e-01],
            [-1.4690e+00,  1.6392e+00,  2.5546e-01,  1.0314e+00, -3.8131e-01,
            -3.0390e-01, -8.4815e-01, -6.9028e-01,  8.8892e-01,  5.1484e-01],
            [-3.1452e-01,  8.1583e-01, -5.1569e-01, -1.6384e-01,  3.1787e-01,
            1.1455e+00, -3.9538e-01,  9.3894e-02,  1.1287e+00,  3.1484e-01],
            [-9.5806e-02, -1.5116e+00, -4.8525e-01,  6.9561e-01,  3.0882e-01,
            -3.5391e-01, -4.6775e-01, -7.9598e-02, -7.8203e-02, -5.5041e-01],
            [-6.3490e-01,  5.2860e-01, -1.2618e+00,  6.6069e-01, -1.0920e+00,
            -6.3054e-01, -1.1858e+00,  3.3993e-02, -7.3483e-01,  1.7397e+00],
            [ 1.6373e+00, -1.4959e+00,  9.5778e-01,  5.3338e-01, -1.4163e+00,
            -1.9292e+00,  4.1678e-01, -9.6391e-01,  6.3254e-01,  7.7971e-01],
            [-5.2291e-01,  3.3317e-01,  7.0726e-01,  2.8153e-01, -1.0492e+00,
            1.9604e-01,  1.3186e+00,  4.6280e-01, -1.8789e+00,  8.7950e-01],
            [-1.9435e+00, -4.7845e-03,  1.2094e+00, -8.0151e-02,  7.9145e-01,
            1.4978e-01, -8.7743e-01,  2.3761e+00,  3.5165e-01, -4.2986e-01],
            [ 6.4396e-02,  1.4085e+00, -6.3705e-01, -1.4974e+00,  1.2425e+00,
            -7.5590e-01,  1.1618e+00, -6.0451e-01, -8.2521e-02, -6.6778e-01],
            [ 6.7537e-01, -7.7506e-01, -1.8521e+00,  5.2576e-02, -8.7476e-01,
            1.6209e+00,  1.0106e+00,  1.2931e+00,  5.3337e-01, -1.0073e-01],
            [-6.8153e-02,  1.4592e-01, -3.5821e-01,  9.9985e-01, -9.4293e-02,
            1.9144e+00, -1.4624e+00,  6.6110e-01,  1.7125e+00, -1.4103e+00],
            [ 9.6939e-01,  1.2639e+00,  3.1307e-01,  5.0962e-01,  1.1887e+00,
            7.7797e-01,  8.8668e-01, -1.0077e+00, -1.1336e+00,  8.6396e-01],
            [ 6.1626e-01,  1.6134e+00, -3.4125e-01,  2.9191e-01,  1.2129e+00,
            -2.5595e-01,  1.0430e+00, -2.0359e-01, -6.5850e-01, -2.8164e-01],
            [-1.0253e+00, -1.0217e+00, -1.0604e+00, -4.2481e-01, -8.0468e-01,
            -1.7862e-01, -5.3114e-01, -1.4525e+00, -5.8505e-02, -6.7154e-01],
            [ 3.6676e-01, -1.1313e+00,  3.2953e-01, -8.0325e-01, -1.8213e-01,
            1.7372e+00, -6.6976e-01, -1.4747e+00, -5.2908e-01, -8.7371e-01],
            [-1.1920e+00, -5.6639e-01, -1.0627e+00,  7.6600e-01,  8.1098e-01,
            -5.7083e-01,  1.3533e+00,  3.5949e-01,  9.1705e-01,  1.2506e+00],
            [-5.7211e-01, -2.2765e-01,  6.4849e-02, -1.4293e+00, -1.4082e+00,
            6.5612e-01, -8.5136e-01,  6.8048e-03, -1.0828e+00,  9.0350e-01],
            [-4.4923e-03,  6.4517e-01, -4.4796e-01, -6.1736e-01, -6.4545e-03,
            -8.8526e-01, -9.9089e-01,  2.4920e-02,  2.8321e-01, -1.1855e+00],
            [ 5.9793e-01, -1.5329e+00, -1.5769e+00, -9.5912e-01,  5.3432e-01,
            5.9364e-01,  1.8058e-01,  7.4152e-01, -8.1262e-01,  9.5024e-02],
            [ 3.1791e-01,  4.4189e-01, -9.8394e-02, -2.5352e-01, -1.0801e+00,
            -1.0470e+00,  6.4284e-01, -1.4315e+00,  1.1955e+00,  1.2444e+00],
            [ 7.8184e-01,  8.3277e-02, -1.9697e-01,  4.5693e-01, -2.9236e+00,
            1.0685e+00,  4.9826e-01,  7.6692e-01,  2.9602e-01,  6.6196e-01],
            [ 1.9042e+00,  7.5868e-01, -6.1698e-01, -1.5544e-01,  3.6938e-01,
            -1.0506e+00,  1.6241e-01,  5.3039e-01,  1.8422e+00,  9.8405e-01],
            [-1.1247e-01, -1.1082e+00,  1.9539e-01, -1.3382e-01, -2.8312e-01,
            -3.0700e-01,  1.9022e+00,  4.2021e-01, -1.0030e+00,  9.2042e-01],
            [ 9.2161e-01, -2.9530e-01,  7.9193e-01,  5.3339e-01, -6.9000e-02,
            8.7510e-01, -6.8746e-01,  3.2069e-01,  6.8680e-01,  1.7246e+00],
            [-3.6245e-01, -2.5166e-01, -6.7368e-01,  1.1523e+00,  5.6932e-01,
            -1.1918e+00,  1.6914e+00,  1.7816e+00, -9.7225e-01,  6.2170e-01]]
         ).to(device)

        # fmt: on
        loss_value = loss(y_pred, y_true)

        loss_value_loop_softmax = cdwce_loop_softmax(y_pred, y_true, alpha, device)
        loss_value_vect = cdwce_vect_softmax(y_pred, y_true, alpha, device)
        loss_value_loop = cdwce_loop(y_pred, y_true, alpha, device)

        assert loss_value.item() == pytest.approx(
            loss_value_loop_softmax.item(), abs=1e-4
        )
        assert loss_value.item() == pytest.approx(loss_value_vect.item(), abs=1e-4)
        assert loss_value.item() == pytest.approx(loss_value_loop.item(), abs=1e-4)
